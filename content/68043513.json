{"question_id": "68043513", "question_text": "最近在看基于机器学习的药物筛选的文章，在一篇基于配体及其蛋白-配体作用关系的筛选中，数据集选用的是数据库（比如pdb，stitch）中的全部靶点及其配体小分子数据，然后表征输入模型，最后利用训练好的模型虚拟筛选某靶点抑制剂。 我的疑问是 我们要筛选某靶点抑制剂，要选用的数据为什么不是 该靶点及其配体小分子数据作为训练集呢？ ", "question_title": "基于机器学习的药物筛选？", "question_commet": "​添加评论", "answer_number": "3 个回答", "question_followers": "95", "brower_number": "2,278"}
{"answer_author": "Hansel", "answer_id": 259110534, "answer_text": "谢邀@左起很好的问题，也属于非常专业的问题。虽然不知道你说的具体是哪篇文章，不过根据你的描述，这其实是个很general的问题，估计很多用机器学习做虚拟筛选的人都会遇到。首先，题主的想法：直接用目标靶点和对应的小分子抑制剂数据作为training set确实是最reasonable的方法，然而有个很大的问题就是：数据量严重不够。很多靶点的小分子抑制剂数量从机器学习需要的数据量上看还很少，虽然我没统计过，但是经验中即使像HIV integrase 这么老的，研究透了的靶点，报道出来的且相关数据可用的抑制剂数量（即便加上同种骨架的不同衍生物）也很难达到上万的级别。对于真正的机器学习来说，这简直不够塞牙缝，所以论文作者选用的是PDB这样的数据库，包含所有不同靶点和对应的配体信息，数据量会更多。然而......一切都有然而......我想说就目前PDB数据库的量和质来说，这样的机器学习模型用于筛选可能仍然比较鸡肋，充其量只能算是一个大型的QSAR吧，我们先看一个数据：<img src=\"https://pic4.zhimg.com/50/v2-c3fc2494326e449a56140ba8967bc6ef_hd.jpg\" data-caption=\"\" data-rawwidth=\"876\" data-rawheight=\"507\" class=\"origin_image zh-lightbox-thumb\" width=\"876\" data-original=\"https://pic4.zhimg.com/v2-c3fc2494326e449a56140ba8967bc6ef_r.jpg\">这是PDB网站的统计，红色是至今为止总共的蛋白晶体结构，蓝色是当年的结构总量。到2017.11.13日为止，总共有120909个结构。而真正在建立虚拟筛选模型时，为了使模型更可信，一般complex分辨率在2.5埃以下会更好，我们看看这个数据：<img src=\"https://pic1.zhimg.com/50/v2-601ed1884751717c9b25aae208140839_hd.jpg\" data-caption=\"\" data-rawwidth=\"435\" data-rawheight=\"283\" class=\"origin_image zh-lightbox-thumb\" width=\"435\" data-original=\"https://pic1.zhimg.com/v2-601ed1884751717c9b25aae208140839_r.jpg\">仍然来源于PDB网站，2.5埃以下的结构总共只有90074个。这9万个结构中还包含很大部分空蛋白（apo）的结构，而且很多complex即使有小分子配体但是活性（Ki/Kd/IC50）并没有报道，所以这么一看蛋白小分子complex总数量更是少得可怜，这样掐头去尾，能用于建模的complex就更少了。做模拟的经常会用到一个优化后的数据库PDB bind（Welcome to PDBbind-CN Database），目前版本2016，总共只有16179个已知活性的蛋白小分子complex，可见目前优秀的数据量有多么稀缺。所以你说一个机器学习方法只用了最多一两万的数据量，可能在真正做机器学习专业的人士看来就是笑话吧。目前大热的机器学习其实就是早年药物研发领域的QSAR，只是那时候数据量更少，效果有限，所以90年代热过一段时间后就沉寂了，目前借着AI浪潮又重新被人想起。不过工业界应用QSAR在化合物毒性预测，ADMET性质预测方面确实产生了很多很好的效果，但这是AI热潮兴起来之前就已经存在的现象了。前几天才刚跟薛定谔公司（药物研发计算软件的开发公司，业界很有知名度）一个应用科学家聊过AI的事，他认为基于这种knowledge的机器学习模型的搭建确实是一个好的思路，然而目前数据的质和量都严重不足，在药物筛选领域想产生真正意义上的颠覆产品可能还为时尚早。个人认为，knowledge-based的虚拟筛选模型的方向应该是应用AI的长处进行规则提取，即从优质数据中发现好的结合特征（feature），让AI生成一个更加准确的打分函数，可能这个打分函数对于人类是unreadable的，但是如果有效，who cares，毕竟药学还是更偏重实际结果，理论的事自然会有人去研究。而目前受困于数据量的深度学习确实在很多领域效果有限，可能仅需较少数据量的新的机器学习方法才有可能是实现颠覆性创新的突破口吧。以上。", "answer_votes": "36", "answer_comment": "​7 条评论"}
{"answer_author": "佚名用户", "answer_id": 261693909, "answer_text": "什么文章这么6？题主可否po出来供我们学习一个？", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "匿名用户", "answer_id": 259371627, "answer_text": "贴出文章题目呗，看看他到底做了啥，怎样做的，或者写信问问ta", "answer_votes": "0", "answer_comment": "​添加评论"}
