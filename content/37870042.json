{"question_id": "37870042", "question_text": "本题已收录至知乎圆桌 » 对弈人工智能，更多关于李世石对战人工智能的解读欢迎关注讨论。", "question_title": "机器学习（machine learning）在经济学领域是否有应用前景？", "question_commet": "​3 条评论", "answer_number": "64 个回答", "question_followers": "5,096", "brower_number": "163,070"}
{"answer_author": "慧航", "answer_id": 78806029, "answer_text": "没有人邀请我，强行来答。我最近正在一家公司兼职，做一些「大数据」与传统银行业相关的事情，在这个问题上还是有很多想法的。当然，由于签了保密协议，具体的业务内容和技术细节恕不透露。按照我回答问题的一贯风格，在回答问题之前，先把「定义」这个问题搞清楚。什么是大数据？我想这个问题不同的人有不同的理解，我理解的大数据有这么几个层次：1、数据量大。这个「大」也有不同层次的理解，要么观测数非常多，要么干脆直接是「全样本」数据。这一点上，对传统的数据存储、计算等都提出了比较高的要求，比如分布式的存储和计算。这是「大数据」的第一个「大」。2、变量多，维数高。这是一个「幸福的烦恼」。对于每一个观测，我们有很多很多维度可以描述清楚这个个体。对于每个人，在社会经济生活中的一举一动，都有可能被某台服务器忠实的记录着，这个潜在的变量规模是不可想象的。甚至，随着文本挖掘、图像识别等技术的进步，我们可以从各种自己想要的维度来挖掘出需要的变量。这是「大数据」的第二个「大」。3、围绕着海量数据、高维数据的数据分析方法。包括但不仅限于传统的统计方法、回归、Logistic、SVM、决策树、聚类、PageRank、神经网络......等各种各样正在飞速发展的数据挖掘方法。以上的理解是我个人的理解，如果有什么不对的地方，欢迎讨论。题主的问题是机器学习在经济学中的应用，但是在这个答案里面，我想更多的讨论一下，「大数据」与经济学的相互影响。先说「大数据」对经济学的影响吧。至少现在来看，虽然越来越多的经济学家开始关注机器学习、大数据，但是对经济学的影响其实有限。但是未来呢？不好说。首先是，机器学习的很多方法本来就是经济学家获得数据的潜在来源。比如文本挖掘的算法，据我所知，已经有人将其应用在了论文中（文章并未公开，所以保密）。这些海量的数据使得经济学的很多领域，比如对社交网络的研究成为了可能。未来当经济学家对某个问题感兴趣的时候，得益于「大数据」的发展，可能更少的受到数据的制约。与此同时，从方法论的角度来看，大数据无疑也会对经济学形成冲击。如果未来的经济学不得不与「大数据」打交道，那么相应的，方法论上必须有相应的发展。传统的经济学最为关注的莫过于识别问题，在「大数据」的条件下，如何仍然能得到清晰的识别？可能在理论上还存在着大量的空白。比如，在Chetty等人的这篇文章：多而无效工具变量下的识别与推断 - EconPaper 中，讨论了当IV随着样本量的增加而增加，且IV都无效的情况下的识别，其实这两个问题，无论是哪一个，都是「大数据」应用在经济学时很可能出现的问题。「大数据」对经济学方法论的影响可能已经开始了。其实我的这个兼职经历让我更想谈的，是经济学在「大数据」上的应用。必须要认识到的是，「大数据」中的很大一部分，都是「人」在社会生活中留下的痕迹，而使用数理框架分析「人」的行为，本来就是经济学的专长。经济学与「大数据」是有本质区别的，比如，经济学更关注「因果推断」以及「识别」问题，而「大数据」似乎对「因果」推断并不感兴趣。比如，《大数据时代》中提出，「更好：不是因果关系，而是相关关系」。我的经验和看法并没有那么的偏激，在业界的应用中，「因果关系」与「识别」并没有那么的重要，然而也并非「相关关系」那么简单。我更喜欢的说法，是《信号与噪声》中的说法，重要的是如何剔除数据中的噪声。噪声可能有很多来源，比较轻微的，比如数据质量问题。很多情况下，在「大数据」与经济学传统的「小数据」之间，存在着一个quantitative-qualitative tradeoff，也就是数据数量与数据质量之间的tradeoff。这种tradeoff本身就存在于经济学中，比如工业企业数据库量大，但是质量还是上市公司的数据质量高；人口普查数据量大，但是质量还是专门的调查质量高。在「大数据」中，为了解决某个问题，会碰到虽然手头上数据很多，但是其实关键变量是缺失的，而其他变量都有不小的噪声。另外一个更严重的问题是，如果缺乏严谨的统计、经济学的培训，可能对数据生成过程（DGP）并不理解，导致结构性的失误。这种噪声带来的就是系统性的误差了。这可能不仅仅是「过拟合」，而是「误拟合」了。举一个再简单不过的例子。有个公司（并非我供职的公司）帮银行做一个甄别理财产品潜在客户的工作，他们使用历史数据，看哪些人更有可能购买理财产品。很简单的，一个Logistic回归也许就能解决问题。银行明确告诉他们，存款余额5w元以下的不考虑，然而公司做出来的结果是，明明余额在5w以下的也有可能购买产品，为啥就不要存款余额5w以下的呢？其实这个问题非常简单，做这个分析的人并没有仔细考虑清楚这个问题的数据生成过程（DGP），并没有真正的理解数据，所以无脑的这样做下来，这样的结果并不奇怪。比如在这里，我们真正关心的变量应该是这个人的「真实财富」，虽然这个人在这家银行的存款余额为5w以下，但是可能这个用户之所以在这里开了储蓄卡，仅仅是因为持有这家银行的信用卡。所以5w以下的客户的确非常有可能购买产品。所以这里的存款余额变量，本来应该是一个「状态变量」，但是实际上，如果仔细考虑DGP，应该是一个「结果变量」。如何排除噪声？可能得建模开卡行为、余额行为才可以。实际上，以上的这些建模思路，都是计量经济学反复培训的，bad control，measurement error, sample selection等等，无一不是在让我们更仔细的思考数据生成过程。在「大数据」中，虽然不重视因果和识别，但是以上问题如果不重视，虽然得到了相关性，但是得到的更多的是被噪声严重污染过的相关性。因为关注的是如何去噪声，而非严谨的因果和识别，因而很多计量上常用的方法，可能也就没有太大必要了。但是计量经济学的建模思路和机器学习的方法结合起来，反而可能收到奇效。比如上面的问题，一个简单的解决思路也许就是结合聚类方法与Logistic回归，就能将模型做的更好。实际上在我的工作过程中，就有过为了剔除噪声，将传统的机器学习、计量经济学的方法重新加以组合而产生的「新方法」。其实总结下来，更精炼的说法就是，经济学、计量经济学的方法，可以在原来的模型中加入新的「结构」，结构的加入可以矫正由于人的种种行为而给数据带来的噪声，解决「过拟合」、「误拟合」的问题，提高模型的精度。所以上海财大经济学院今年新开了一个「大数据经济学」的专业。我并不是非常了解这个专业的具体培养方式，但是我个人觉着是个非常不错的尝试。以上纯属个人看法。", "answer_votes": "455", "answer_comment": "​105 条评论"}
{"answer_author": "匿名用户", "answer_id": 84339008, "answer_text": "转载两篇最近刚看到的科普文，作者是美国的“明星”经济学家 Susan Athey。&lt;img src=\"https://pic2.zhimg.com/50/9a9b4bbd2b2f34cd03ba547d01e4efcf_hd.jpg\" data-rawwidth=\"1920\" data-rawheight=\"1152\" class=\"origin_image zh-lightbox-thumb\" width=\"1920\" data-original=\"https://pic2.zhimg.com/9a9b4bbd2b2f34cd03ba547d01e4efcf_r.jpg\"&gt;(Source: Google DeepMind)What will be the impact of machine learning on economics?From Quora by Susan Athey on January 27, 2016- Source: https://www.quora.com/What-will-be-the-impact-of-machine-learning-on-economicsThe short answer is that I think it will have an enormous impact; in the early days, as used “off the shelf,” but in the longer run econometricians will modify the methods and tailor them so that they meet the needs of social scientists primarily interested in conducting inference about causal effects and estimating the impact of counterfactual policies (that is, things that haven’t been tried yet, or what would have happened if a different policy had been used).  Examples of questions economists often study are things like the effects of changing prices, or introducing price discrimination, or changing the minimum wage, or evaluating advertising effectiveness.  We want to estimate what would happen in the event of a change, or what would have happened if the change hadn’t taken place.As evidence of the impact already, Guido Imbens and I attracted over 250 economics professors to an NBER session on a Saturday afternoon last summer, where we covered machine learning for economists, and everywhere I present about this topic to economists, I attract large crowds. I think similar things are true for the small set of other economists working in this area. There were hundreds of people in a session on big data at the AEA meetings a few weeks ago. Machine learning is a broad term; I’m going to use it fairly narrowly here. Within machine learning, there are two branches, supervised and unsupervised machine learning. Supervised machine learning typically entails using a set of “features” or “covariates” (x’s) to predict an outcome (y). There are a variety of ML methods, such as LASSO (see Victor Chernozhukov (MIT) and coauthors who have brought this into economics), random forest, regression trees, support vector machines, etc. One common feature of many ML methods is that they use cross-validation to select model complexity; that is, they repeatedly estimate a model on part of the data and then test it on another part, and they find the “complexity penalty term” that fits the data best in terms of mean-squared error of the prediction (the squared difference between the model prediction and the actual outcome). In much of cross-sectional econometrics, the tradition has been that the researcher specifies one model and then checks “robustness” by looking at 2 or 3 alternatives. I believe that regularization and systematic model selection will become a standard part of empirical practice in economics as we more frequently encounter datasets with many covariates, and also as we see the advantages of being systematic about model selection. Sendhil Mullainathan (Harvard) and Jon Kleinberg with a number of coauthors have argued that there is a set of problems where off-the-shelf ML methods for prediction are the key part of important policy and decision problems. They use examples like deciding whether to do a hip replacement operation for an elderly patient; if you can predict based on their individual characteristics that they will die within a year, then you should not do the operation. Many Americans are incarcerated while awaiting trial; if you can predict who will show up for court, you can let more out on bail. ML algorithms are currently in use for this decision in a number of jurisdictions. Goel, Rao and Shroff presented a paper at the AEA meetings a few weeks ago using ML methods to examine stop-and-frisk laws. See also the interesting work using ML prediction methods in the session I discussed on “Predictive Cities”: 2016 ASSA Preliminary Program where we see ML used in the public sector. Despite these fascinating examples, in general ML prediction models are built on a premise that is fundamentally at odds with a lot of social science work on causal inference. The foundation of supervised ML methods is that model selection (cross-validation) is carried out to optimize goodness of fit on a test sample. A model is good if and only if it predicts well. Yet, a cornerstone of introductory econometrics is that prediction is not causal inference, and indeed a classic economic example is that in many economic datasets, price and quantity are positively correlated. Firms set prices higher in high-income cities where consumers buy more; they raise prices in anticipation of times of peak demand. A large body of econometric research seeks to REDUCE the goodness of fit of a model in order to estimate the causal effect of, say, changing prices. If prices and quantities are positively correlated in the data, any model that estimates the true causal effect (quantity goes down if you change price) will not do as good a job fitting the data. The place where the econometric model with a causal estimate would do better is at fitting what happens if the firm actually changes prices at a given point in time—at doing counterfactual predictions when the world changes. Techniques like instrumental variables seek to use only some of the information that is in the data – the “clean” or “exogenous” or “experiment-like” variation in price—sacrificing predictive accuracy in the current environment to learn about a more fundamental relationship that will help make decisions about changing price. This type of model has not received almost any attention in ML. In some of my research, I am exploring the idea that you might take the strengths and innovations of ML methods, but apply them to causal inference. It requires changing the objective function, since the ground truth of the causal parameter is not observed in any test set. Statistical theory plays a bigger role, since we need a model of the unobserved thing we want to estimate (the causal effect) in order to define the target that the algorithms optimize for. I’m also working on developing statistical theory for some of the most widely used and successful estimators, like random forests, and adapting them so that they can be used to predict an individual’s treatment effects as a function of their characteristics. For example, I can tell you for a particular individual, given their characteristics, how they would respond to a price change, using a method adapted from regression trees or random forests. This will come with a confidence interval as well. You can search for my papers on arXiv.org e-Print archive; I also wrote a paper on using ML methods to systematically asses the robustness of causal estimates in the American Economic Review last year. I hope that some of these methods can be applied in practice to evaluate randomized controlled trials, A/B tests in tech firms, etc. in order to discover systematically heterogeneous treatment effects. Unsupervised machine learning tools differ from supervised in that there is no outcome variable (no “y”): these tools can be used to find clusters of similar objects. I have used these tools in my own research to find clusters of news articles on a similar topic. They are commonly used to group images or videos; if you say a computer scientist discovered cats on YouTube, it can mean that they used an unsupervised ML method to find a set of similar videos, and when you watch them, a human can see that all the videos in cluster 1572 are about cats, while all the videos in cluster 423 are about dogs. I see these tools as being very useful as an intermediate step in empirical work, as a data-driven way to find similar articles, reviews, products, user histories, etc.Is ML \"just\" prediction? What can tech companies and traditional ML take from economists' focus on causal inference? From Quora by Susan Athey on January 27, 2016 - Source: https://www.quora.com/Is-ML-just-prediction-What-can-tech-companies-and-traditional-ML-take-from-economists-focus-on-causal-inferenceThis is a great question.  I covered a lot of this in my answer to another Quora question: What will be the impact of machine learning on economics?I think that \"just prediction\" can be more important than you think in social science, but ultimately I think that methods need to modified somewhat to give the best answers for causal questions.  ML methods optimize for an objective; you need to pick the RIGHT objective for your question, and minimizing mean-squared error on a test sample is often not the right one. ML practitioners might do well to learn more about instrumental variables and other techniques for causal inference; in the small data setting, my husband Guido Imbens together with Don Rubin have a great 2015 book summarizing statistical methods for causal inference developed over the last few decades. In tech firms, a problem like click prediction is ultimately about counterfactuals.  How many clicks WOULD a link get if you put it in the top position?  I think that these models might be improved by incorporating insights from the econometrics literature about how to estimate causal effects and simulate counterfactuals; although those models can't be used off the shelf at the scale tech firms need, and further economists have a standard of \"perfection\" for causal inference that probably can't be attained in practice. ", "answer_votes": "78", "answer_comment": "​14 条评论"}
{"answer_author": "钱粮胡同", "answer_id": 92024325, "answer_text": "上个做过的案例供讨论。这篇文章是working paper，几年前写了后就放在那里了，是我写的一本书的一部分（A Disaggregate Analysis of China's Regional Development）。如果有同行在这个领域有兴趣，可以看看是否摘出一部分一起写个中文的发表。插播：文中热感地图的制作请参看我的另一篇回答：怎么在 Excel 上做数据地图？ - 钱粮胡同的回答声明：本人是经济研究领域，机器学习只能说是初学者。当时是在研究中国区域间收入不平等的时候遇到一些问题，所以考虑引用机器学习来尝试开拓新的思路。这里想说的是，无论是机器学习或是传统计量，模型与方法可以很复杂，但是研究的课题仍然可以很传统。不能因为想用机器学习而用，一切研究的出发点应该是学科本身的问题，循序渐进，不能忘本。数据：对于机器学习本身，样本量不大，个人觉得这也是某些经济学领域使用机器学习的限制。中国200多个地级市与区域的50个经济，社会发展，交通，通信与金融等基础设施的变量，时间跨度1992年到2008年。基本所有数据都可以从国家统计局获得（数据的获取可参考：从事经济、金融工作的人都是通过什么渠道获得数据资源，运用什么软件来分析行业状态和经济走势的？ - 钱粮胡同的回答）。使用机器学习的原因：开始是因为想看看中国省间与省内最新的GEM指标（广义熵，可分解），然而一方面部分学者已经做过相关的分析，同时我个人觉得局限于省级行政划分的收入不平衡比较传统不够灵活而且高度概括；很多时候人们看收入 (或经济发展程度) 的不平衡不光是跟附近城市的比较，而会说比如一个省会城市的居民，他自己的收入会跟其他省会或者直辖市的比较，看看差距在哪里。因此，打散中国的行政划分，以更小的行政区域为单位（比如地级市 - 因为这里不仅追求空间单位上要尽量小，还要考虑数据的可获得性）重新组合出不同的空间cluster，分析这些cluster间的收入差距就变得有意思了，或者说起码结果会有一定启发性。于是，先做一个简单的散点图，看看省内地级市之间的收入差距和driver：&lt;img src=\"https://pic4.zhimg.com/50/bf09531239c870010ec502f21d1b64ec_hd.jpg\" data-rawwidth=\"502\" data-rawheight=\"373\" class=\"origin_image zh-lightbox-thumb\" width=\"502\" data-original=\"https://pic4.zhimg.com/bf09531239c870010ec502f21d1b64ec_r.jpg\"&gt;这张图简单的解释就是：垂直虚线的左边的城市群基本都是省会城市或单列市，而这些城市是导致省内收入不均的主要因素。由此，我的基本假设是省间收入的不平衡是长久的话题，而省内的不同质其实已经逐渐超过省间。基于这点，我想采用神经网络 (artificial neural network) 的一种，Self-organizing Map (SOM)，来帮我重新分类中国城市（尽量同质），而我不告诉这个系统我的分类标准是什么，我需要做的只是设置学习参数，把整理干净 (normalized) 的数据扔进去。这个模型 (SOM-Ward-clustering, quantization error ~0.0026) 的好处是可以生成漂亮的热感地图，有助于分析理解，尤其是这种地理经济类的研究。引用网上的一个过程图解释SOM的算法（庞杂的数据点逐步unfold到二维分布图上）：&lt;img src=\"https://pic1.zhimg.com/50/b5bb6a64da2dc9ed080a142557588163_hd.jpg\" data-rawwidth=\"850\" data-rawheight=\"147\" class=\"origin_image zh-lightbox-thumb\" width=\"850\" data-original=\"https://pic1.zhimg.com/b5bb6a64da2dc9ed080a142557588163_r.jpg\"&gt;最终SOM的结果：&lt;img src=\"https://pic2.zhimg.com/50/ecd2ec5c5e0600932221b978188904ac_hd.jpg\" data-rawwidth=\"461\" data-rawheight=\"223\" class=\"origin_image zh-lightbox-thumb\" width=\"461\" data-original=\"https://pic2.zhimg.com/ecd2ec5c5e0600932221b978188904ac_r.jpg\"&gt;可以看到样本城市群被分为三类，黄色代表较高发展程度的城市群，粉色代表中等，蓝绿色代表较低。之后的任务就是根据每个集群的属性特点，用经济学解释，编故事写文章了。好玩的是，我们还可以把这些重新分类的城市群画成地级市级别的中国地图，这样就更显而易见，有利于经济学分析与解释：&lt;img src=\"https://pic3.zhimg.com/50/440af99809b8f062a1139e980506232b_hd.jpg\" data-rawwidth=\"523\" data-rawheight=\"397\" class=\"origin_image zh-lightbox-thumb\" width=\"523\" data-original=\"https://pic3.zhimg.com/440af99809b8f062a1139e980506232b_r.jpg\"&gt;", "answer_votes": "93", "answer_comment": "​12 条评论"}
{"answer_author": "扣小米", "answer_id": 76442595, "answer_text": "我的专栏——机器学习、大数据与经济学研究 - 大石头路73号 - 知乎专栏对于这个问题，经济学大牛 Varian 已经写论文说过了，知乎上有人给了论文链接，我在这里简单介绍一下这篇文章的内容。\nVarian, 2014, Big data: New tricks for econometrics\n这里有一篇中文的介绍：\n【香樟论坛】大数据：计量经济学的新技巧其实这篇介绍的挺详细。但如果我全都照搬过来，岂不是很没有创造性？所以我决定重新写一篇。。。\n论文作者是范里安（Hal Varian），学过经济学的应该都知道这位大名鼎鼎的美国经济学家，著有经济学教材《微观经济学：现代观点》，就算没读过应该也听说过。他现在已经从加州大学伯克利分校退休，现任 Google 首席经济学家，参与设计了 Google 广告关键词拍卖系统等项目。\n范里安为 Google 设计的「AdWords Select」系统背后的经济学原理是怎样的？ - 谷歌 (Google)首席经济学家范里安：谷歌摇钱树 -- 经济金融网 -- 传送门看看他在这篇论文脚注中的作者介绍，感觉还是挺酷的：\nHal Varian is Chief Economist, Google Inc., Mountain View, California, and Emeritus Professor of Economics, University of California, Berkeley, California.不知道是不是受到 Google 程序员们的影响，范里安现在对机器学习和大数据很感兴趣。\n范里安认为，计算机技术现在已经深入到经济学研究中。传统的统计和计量方法，比如回归分析，当然是不错的研究方法，但如今数据量越来越大，而正好符合研究要求的数据已然有限，同时大数据量让变量之间的关系变得更加灵活，传统计量中的线性以及大多非线性模型可能都无法满足这一要求，所以经济学家需要寻找新的研究方法。范里安认为，机器学习理论中的决策树（decision trees），support vector machines，深度学习（deep lerning）等技术，可以更加有效率的处理复杂的关系。\n所以，他在文中的思路可以简单总结为：\n- 经济学要与数据打交道，传统分析用的是样本等小数据\n- 随着经济交流的日益频繁和技术水平的提高，数据越来越大，大数据出现\n- 传统经济学分析方法在分析大数据时显得捉襟见肘\n- 我们需要新的分析方法\n- 机器学习技术可以在这方面帮助我们\n这篇文章开始给读者介绍了一些处理数据的方法和软件，以及大型 IT 公司的处理方法，这还是挺有用的。比如在处理百万条的大型数据时需要用到 SQL，数据清理可以用 OpenRefine 和 DataWrangler。\n不过计量经济学和机器学习当然是有区别的，作者认为：Data analysis in statistics and econometrics can be broken down into four categories: 1) prediction, 2) summarization, 3) estimation, and 4) hypothesis testing. Machine learning is concerned primarily with prediction.[...]Machine learning specialists are often primarily concerned with developing high-performance computer systems that can provide useful predictions in the presence of challenging computational constraints.[...]Data science, a somewhat newer term, is concerned with both prediction and summarization, but also with data manipulation, visualization, and other similar tasks.计量和统计学主要关注四个方面：预测、总结、估计和假设检验。机器学习主要关注预测。数据科学侧重预测和总结，也涉及数据处理、可视化等。\n计量经济学关注因果关系，会遇到内生性等问题，而机器学习则会遇到“过度拟合”（overfitting）的困扰，但机器学习可以关注到计量和统计中样本以外的数据。\n那么机器学习如何运用到经济学中呢？作者举了几个例子。\n一个是分类和回归树分析（Classification and regression trees，简称CART），这一方法适用于分析一件事情是否发生以及发生概率的时候，即被解释变量是0或1。计量上通常用 logit 或 probit 回归。\n范里安这里用的是例子是泰坦尼克号沉船事件中不同人群的死亡概率。作者用机器学习理论中的 CART 方法（R 软件中有这个包 rpart），把船上的乘客按照舱位等级和年龄进行分类。\n&lt;img src=\"https://pic3.zhimg.com/50/c6d8faff8ff767f8bad067b5df79cdc7_hd.jpg\" data-rawwidth=\"940\" data-rawheight=\"298\" class=\"origin_image zh-lightbox-thumb\" width=\"940\" data-original=\"https://pic3.zhimg.com/c6d8faff8ff767f8bad067b5df79cdc7_r.jpg\"&gt;这是树模型（Tree model）的分类，舱位分一、二、三等，一等最好，三等最差。然后做成树型的样式：\n&lt;img src=\"https://pic3.zhimg.com/50/d08e35c957ffe70f6d27a971b30c7f80_hd.jpg\" data-rawwidth=\"700\" data-rawheight=\"636\" class=\"origin_image zh-lightbox-thumb\" width=\"700\" data-original=\"https://pic3.zhimg.com/d08e35c957ffe70f6d27a971b30c7f80_r.jpg\"&gt;最上面一层把乘客按照舱位分开，左边是三等舱，右边是一等和二等。三等舱（很有可能穷人居多）死亡概率较高，501个人中有370个遇难。接下来把右边一等和二等的乘客按照年龄分类，左边是大于等于16岁的，右边是小于16岁的儿童。先看儿童，这类人群的幸存概率很高，36个人中有34个都活下来了。左边把年龄16岁及以上的人又分为两类，左边的二等舱和右边的一等舱。二等舱233人中有145人遇难，一等舱276个成年人中174人幸存下来。我算了一下，四类人从左到右的幸存概率分别是26%、37%、63%和94%。所以在泰坦尼克沉船时，儿童和一等舱的人容易活下来。\n接下来重点关注一下乘客的年龄分布，下图是各年龄段的幸存概率以及置信区间：\n&lt;img src=\"https://pic1.zhimg.com/50/0c64264266452bdc1990adf31e0487b6_hd.jpg\" data-rawwidth=\"1280\" data-rawheight=\"756\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic1.zhimg.com/0c64264266452bdc1990adf31e0487b6_r.jpg\"&gt;从图中可以看出，10岁所有的儿童和60岁左右的人幸存概率更高。\n同时，作者又用传统的计量方法 logit 模型回归了一下，解释变量是年龄，被解释变量是幸存（1）。结果如下：\n&lt;img src=\"https://pic3.zhimg.com/50/4f1d12d3fb788aaf2f4301472efbba31_hd.jpg\" data-rawwidth=\"940\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"940\" data-original=\"https://pic3.zhimg.com/4f1d12d3fb788aaf2f4301472efbba31_r.jpg\"&gt;年龄（age）与幸存为显著的负相关，即年龄越小越可能在沉船时活下来，但是系数太小，影响很弱。总结这两种方法作者认为，是否幸存并不取决于年龄，而是乘客是否是儿童或者60岁左右的人，这一点在回归分析中无法反映出来。\n类似的机器学习的方法还有一个叫conditional inference tree，这里同样是运用泰坦尼克的数据制作的图：\n&lt;img src=\"https://pic2.zhimg.com/50/09b1e2345cf2074b5f7fc25079221c42_hd.jpg\" data-rawwidth=\"1288\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb\" width=\"1288\" data-original=\"https://pic2.zhimg.com/09b1e2345cf2074b5f7fc25079221c42_r.jpg\"&gt;这幅图把乘客进行了更加细化的分类，添加了性别(sex)一项。pclass 是舱位登记，age 年龄，sibsp 为船上兄弟姐妹和配偶的数量。最下面的刻度中黑色是这一人群的幸存比例。可以得出的结论是妇女和儿童的幸存率最高。（这是因为在沉船的时候大家大喊“让小孩和女人先走”吗。。？）\n上面这个例子比较简单，但也足够明了，我认为它比较清楚的解释了机器学习和计量的差别：机器学习更加关注相关性和预测，所以得出的结论是某个年龄段的人群幸存率更高。而计量更加关注因果关系，根据上面的 logit 模型，很难说是年龄导致了幸存，很明显还有很多其他变量没有被考虑进去，比如个人体质等等，或许年龄根本就不是计量经济学家在这里主要考察的变量。而且如果加入交叉项也许会有新的结论。所以简单的说就是模型设定的问题。具体哪种方法更好，还要看具体研究的问题是什么。此外，作者还举了其他机器学习的例子，如 boosting, bagging, bootstrap, bayes，这里就不详细说了。文中的几个例子挺值得一看，可以了解一下机器学习的基本方法。比如利用机器学习研究一家公司投放广告是否有效，传统的计量方法是需要设计实验，设立处理组和对照组，但成本较高。\n范里安认为过去几十年计算机科学家与统计学家已经进行了许多卓有成效的合作，他对机器学习在经济学，特别是计量经济学和统计学的应用十分看好，所以范里安给经济学专业的学生一条建议：\n[…] my standard advice to graduate students these days is “go to the computer science department and take a class in machine learning.“学经济学的都去计算机系修一下机器学习的课程吧！”\n范里安一方面是传统意义上的经济学家，另一方面由于在 Google 工作的经历，使得他对机器学习技术有了更深的了解，他的观点值得一看。\n当然，这也只是他的一家之言，毕竟机器学习和计量经济学在方法、目的上都有较大差别，机器学习是否会真的大范围进入到经济学领域还很难说。不过，计算机技术越来越多的被运用到经济学研究当中已经是不争的事实，而且也是趋势，现在如果不懂点编程技术（至少是计量软件），很难在经济学这个行当走的更远。虽然现在有不少功成名就的经济学家和教授在写代码方面并不精通，但每个时代对人们的要求都有不同，当年计算机技术还没有普及，而他们接受的教育实际上已经比之前的人有了长足进步。我想，今天这个时代对经济学研究者的要求之一就是掌握一定的编程技术吧，虽然不用达到写应用程序那个等级，但也得有较高的搜集数据和处理数据的能力。\n所以，如果你只是打算学完经济学就去公司当白领，那么写代码技术可能不是必需的，反而用好word, excel, powerpoint 可能更有用（IT、金融等对写代码有一定要求的行业或岗位除外）。但假如想在学术圈待下去并且有所建树，那么从长远考虑，现在就赶紧去学点编程技术，提高自己的数据处理能力吧。\n现在网上这类资源很多，比如coursera上John Hopkins大学很有名的数据科学的课程\nhttps://www.coursera.org/specializations/jhudatascience?utm_medium=courseDescripTop大部分都带中文字幕\n我自己也在听这个公开课。另外我也会在YouTube找一些数据处理的课程，YouTube 在这方面的优点是资源很丰富（中国的视频网站在这方面的资源太匮乏），基本上只要你能想到的软件教程都会有。缺点是质量参差不齐，而且有些视频不完整，有的视频发布者更新一段时间就停止了。相比而言，coursera 的质量和完整度都要更胜一筹。所以 coursare 和 YouTube 结合起来还是不错的。\n其他参考资料：\n计量经济学、时间序列分析和机器学习三者有什么区别与联系？ - 经济学", "answer_votes": "309", "answer_comment": "​15 条评论"}
{"answer_author": "李佳飞", "answer_id": 75799485, "answer_text": "【3月28日更新】最近逛Quora的时候看到斯坦福的教授Jonathan Levin也被问到了这个问题，拟译如下：【问题】从短期（2-5年），以及更长远的角度来看，机器学习将如何影响经济学？【回答】Machine learning methods are really powerful for fitting predictive models and for doing classification on large-scale, high-dimensional data. These are the data we increasingly use in economics. So I think there’s no doubt many machine learning methods will get used more and more often.机器学习方法强大的地方在于拟合预测模型，还有对高维度的大数据进行分类。而这种类型的大数据，我们在经济学中正在越来越多地应用。所以我认为毫无疑问， 机器学习方法会被越来越频繁地使用。One area that's going to get a lot of attention is combining machine learning with causal inference. A big fraction of empirical microeconomics is about finding ways to exploit natural experiments, whether by using instrumental variables, regression discontinuity, matching, difference-in-difference estimators, or other methods.未来有一个领域将会得到很多的关注，那就是将机器学习与因果推理相结合。实证微观经济学有很大一部分是寻找不同方法来利用自然实验，不管是使用工具变量，回归的不连续性，匹配，倍差法，还是别的研究方法。Large-scale data has great advantages in terms of finding natural experiments (to take a trivial example, if you want to measure how a July 15 price change affected sales, it’s much more powerful to have daily sales data than monthly sales data). But for the most part economists trying to estimate causal models on large-scale data are using traditional methods like fixed effects linear regression. Having some easy to use alternatives would probably make a significant difference in empirical research. 而大数据的好处在于发现自然实验。举个简单的例子，如果你想要测量七月十五日的价格变动如何影响销售，那么拥有日销售数据就比月销售数据更加有用。不过大多数经济学家在使用大数据测量因果模型时，还在使用传统的方法，比如固定效应线性回归。 有一些易于使用的替代选项很可能会给实证研究带来显著的影响。I actually think one way machine learning (or really, more data) will affect the field is that after a while it will re-energize economic theory. The reason is that we are going to generate all kinds of new interesting facts – about individual behavior, labor markets, firm productivity, the macro-economy – and having a bunch of new and possibly disconnected or contradictory facts makes a great starting point for new models and theories.实际上，我认为机器学习（或者说，更多的数据）改变这个学科的方式之一是为经济理论研究重新注入能量。原因是，我们能够产生出各种新奇有趣的事实发现——关于个体行为，劳动力市场，企业生产力，宏观经济——而大量看似脱节甚至矛盾的发现，也可以成为新模型和新理论的出发点。【原帖链接】How will machine learning affect economics?【原答案】借用Hansen大神的句式，我猜Machine Learning在经济学中的应用前景大概是：『It allows you to do something without having to understand anything.』注：Lars Hansen 谈起自己的研究时，往往会这样概括：『It allows you to do something without having to do everything.』Lars Hansen: Doing Something Without Doing Everything", "answer_votes": "201", "answer_comment": "​15 条评论"}
{"answer_author": "王喆", "answer_id": 114167050, "answer_text": "机器学习在经济学领域最大的应用前景必然是量化投资。基于大数据和机器学习的互联网征信固然重要，但毕竟只是一个支持性的领域，不能直接产生收益。基于大数据和机器学习的宏观经济分析也很重要，但也只是提供决策建议。又有什么能比机器学习直接影响投资决策，直接控制几十亿甚至上百亿的资金更有吸引力的呢？前段时间去对外经贸大学做过一次分享，介绍了国内几只大数据基金的框架和原理，已经算得上机器学习和大数据在公募量化投资领域第一步的尝试，再次跟大家分享出来，希望大家能体会一下机器学习在量化投资的应用前景。一、模型框架<img src=\"https://pic1.zhimg.com/50/6e335dd36f897f813e4bb16436acea5e_hd.jpg\" data-rawwidth=\"1484\" data-rawheight=\"1110\" class=\"origin_image zh-lightbox-thumb\" width=\"1484\" data-original=\"https://pic1.zhimg.com/6e335dd36f897f813e4bb16436acea5e_r.jpg\">第一张介绍了传统量化多因子选股模型个部分的组成，以及对应的机器学习工程的各个阶段。全部A股：样本空间全部A股到选样空间：ETL过程选样空间到初筛股票池：基于规则的样本过滤多因子提取：特征工程量化引擎：模型构建与训练量化引擎到指数成分股：模型应用可以看到多因子选股模型已经是一套比较完备的机器学习系统了，在各个阶段，使用不同的模型和算法，就成为了不同的量化模型。这里着重介绍一下特征工程阶段所选的特征，传统的多因子模型所采用的因子主要包括两大类：财务因子（市盈率、市净率、市销率、资产市值比、主营业务收入增长率、净利润增长率、EPS 增长率、总资产增长率等）市场驱动因子（选取短期收益率、长期收益率、特定波动率、交易量变化、自由流通市值）。二、大数据基金的特点<img src=\"https://pic3.zhimg.com/50/5d04951e35c419d816c07e672c9fd380_hd.jpg\" data-rawwidth=\"1480\" data-rawheight=\"1114\" class=\"origin_image zh-lightbox-thumb\" width=\"1480\" data-original=\"https://pic3.zhimg.com/5d04951e35c419d816c07e672c9fd380_r.jpg\">大数据基金对于机器学习和大数据进行了进一步的应用，主要也在于特征工程这一步，区别于传统特征工程中仅采用“财务因子”和“市场驱动因子”，大数据因子的范围非常广泛，可以说只要有能够量化的影响一只股票的因素，都可以抽象成大数据因子。下面就给大家列一下国内目前几只大数据基金采用的非常有意思的特征：淘宝大数据100基于淘宝相关行业的选样空间，博时基金与蚂蚁金服生成了“聚源电商大数据因子”用于多因子量化模型的选股。其中支付宝金融信息服务平台提供网上消费类统计型趋势特征数据。根据所得行业投研指标，综合考察行业的景气度,包括：成长、价格、供需情况等,得到行业景气度排名。进而根据景气度对行业内股票给予相应评分，得到聚源电商大数据因子得分。百发100指数——搜索因子对样本空间的股票分别计算最近一个月的搜索总量和搜索增量，分别记为总量因子和增量因子；对搜索总量因子和增量因子构建因子分析模型，计算每期个股的综合得分，记为搜索因子；雪球智选大数据100——雪球热度因子首先,根据第二步得到的雪球智选组合,计算待选样本的智选组合覆盖度;其次,根据个股的智选组合覆盖度,对股票给予相应评分,记为个股的雪球热度因子得分。南方新浪大数据——新浪大数据因子新浪财经频道下的页面点击量,微博的正负面文章报道、新闻报道影响。银联大数据指数——银联行业大数据因子基于银联消费类统计型趋势特征数据经加工得到行业投研指标；其次，根据所得行业投研指标，综合考察行业的景气度，包括：消费金额、交易次数等，得到行业景气度排名；最后，根据景气度对行业内股票给予相应评分，得到行业大数据因子得分。从上面大数据因子的选择我们就可以看到，这些特征本身都是分值类特征，特征生成的过程就用到了机器学习的模型，而这些因子又作为量化选股模型的输入。三、量化引擎的选择<img src=\"https://pic2.zhimg.com/50/a78a91d65a5d2bd10e41aacfb96963be_hd.jpg\" data-rawwidth=\"1476\" data-rawheight=\"1108\" class=\"origin_image zh-lightbox-thumb\" width=\"1476\" data-original=\"https://pic2.zhimg.com/a78a91d65a5d2bd10e41aacfb96963be_r.jpg\">对于多因子选股模型来说，量化引擎部分当然就是我们说的机器学习模型了。选股模型既可以是一个回归问题——在最后的成分股中选出得分最高的N支股票；也当然可以是一个分类问题——选出最接近目标的那个股票分类组。所以各种机器学习模型也都各显神通。Adaboost 当然是在这个场景中最直观的模型了，因为各个因子本身就是一个弱分类器，大数据因子更是一个较强的分类器，如何将这些弱分类器融合在一起成为一个强分类器就是Adaboost的使命了。Logistic Regression 这种万能的算法模型当然也是适用的，将每个因子看作一个feature，然后得到一个score这种事情，Logistic Regression是最拿手的，但对于金融数据来说，样本量毕竟太少，更适合解决大样本简单问题的Logistic Regression用在选股这种实际问题中总归还是有点naive。。SVM 作为史上最强分类器，当然是解决这种小样本复杂问题的利器，于是各家的量化选股模型确实有不少采用SVM的。肯定还有很多人想起DNN，HMM等模型，但很遗憾，稍微分析就可以知道在量化选股这个实际问题上这类模型并不那么适用，还是用在高频、择时、趋势等问题的解决上比较好，在此就不再展开了。以上的经验有我在解决实际问题的一些积累，更多的还是跟金融行业的同学交流讨论的结果，但更多的技术细节就不便透露了。说了这么多，我觉得大家肯定能够体会到，机器学习在经济金融领域最激动人心的应用还是在于量化投资，还是那句话，又有什么能比机器学习直接影响投资决策，直接控制几十亿甚至上百亿的资金更有吸引力的呢？最后还是要做个广告啊，想和我们更多交流，欢迎关注我的微信公众号「科学投资」：kexuetouzi", "answer_votes": "139", "answer_comment": "​15 条评论"}
{"answer_author": "胖骁", "answer_id": 78901441, "answer_text": "题目说的是经济学中的应用前景，不是量化投资中的应用，很多人上来偏题了，经济学和量化投资的差别显而易见。业界赚钱即可，但我认为经济学的意义在于解释和制定政策。我的观点是有一定的应用但是非常有限，并且已经挖不出什么东西来了。原因如下：1. 机器学习在经济学和金融学的学术论文里出现的越来越多了。其实如果把线性回归，logistic回归和简单的神经网络也当做机器学习的话，那很早以前就有了。但是如同很多答案里写到的，机器学习注重相关性，而经济学往往是注重因果性的，机器学习模型并不能给出因果性。经济学数据很多是时间序列，时间序列具有天然的因果性。2.简单的神经网络（2层）以及logistic回归，都具有可解释性的，而SVM以及深度学习这种模型都是难以解释的。决策树类模型虽然可以给出一个清晰的分类顺序，但是第一，树类模型往往没找到最优解，今年informs上Bertsimas提出了optimal tree，但是第二，即便找到了最优解，决策树给出的模型给出的分类法则也会出现匪夷所思的分类方法。3.据我所看过的几篇非top的论文来看（如果有关于机器学习的论文发在econ top 5上还望告知），机器学习模型在经济学的应用存在‘套’的情况，即选定一揽子模型，每个模型又有几种kernel，在不同的参数环境上测，然后选出最好的一个模型一组参数，说你看机器学习是有用的。而传统的经济学论文呢？提出一个模型，然后在一个参数环境下测，结果还不错，用其他的参数环境测试稳健性，发现稳健，说明现象存在。虽然一篇经济学论文在前期一样要去试模型，但是试模型的范围要小得多，每一个模型都是有逻辑的。另外从成文的思路上看，也可以看出经济学的逻辑和机器学习是不一样的。4.经济学不光是分类和推断。毕竟你不能指望机器学习告诉你最佳的基准储备金率。5.说完不适合的地方再说说我见过的应用，以下都指发出过论文的应用。首先是过去20年不断出现的拿机器学习做交易策略的paper，这些paper有一个共同的特点，就是模型简陋，用的数据非常敏感还装模作样的做cross validation...毕竟如果真能赚钱他们还发什么paper┑(￣Д ￣)┍第二种是做非时序的，比如衡量资产风险，这种模型往往比较稳健。由于经济学的论文是需要insight的，所以往往会对因子的重要程度做个排序，当然这种分析跟传统计量的显著性检验等等相比感觉没什么说服力。6.所以综上所述，其实机器学习能给出的经济学insight很有限，大多数论文有灌水的嫌疑，而且这种灌水的机会越来越少了。今年informs上Stanford MSE的教授Kay Giesecke有一个Large-Scale Loan Portfolio Selection，已经用了深度学习+GPU计算，得出来的结果跟简单机器学习比，在大多数情况下也就提升了3-5%的准确率吧。。。 所以我个人是很不看好机器学习在经济学上的应用的，当然在业界管你用什么方法赚钱呢O(∩_∩)O~", "answer_votes": "100", "answer_comment": "​15 条评论"}
{"answer_author": "王也", "answer_id": 129793099, "answer_text": "最近看来发展非常迅猛，于是决定答一下这道题……其实机器学习的一些简单的想法早就被吸收到计量经济学里了，什么splines啊，k-fold cross validation啊这些，一直有人在用。但这几年随着Athey和Imbens等人的力推确实吸引了越来越多人的眼球。目前的应用主要是两方面，计量理论上的和实证上的。理论上主要就是把预测的部分都改用机器学习来实现，典型的例子比如估计propensity score，估计工具变量的一阶段。因为这些环节不涉及因果性，只要预测准确就好，所以机器学习就特别有优越性。现在比较热门的话题是用机器学习来估计潜在结果Y(0)，比如在实验里面通过哪些协变量来预测控制组的结果，Lasso在这个环节就大显身手了。还有一个方向是用机器学习来看处理效应（ATE）的异质性，Athey和Imbens之前有一篇文章，普林斯顿的政治学家Imai也做过类似的工作，可以计算如何分配资源以便让竞选活动的效果最大化。实证研究里机器学习往往是起到辅助的作用。比如Olken和他的四个合作者2012年发的一篇QJE，通过卫星照片测量印尼的森林砍伐率。方法就是先找一帮人手动把一部分照片上不同颜色的区块归到各个类别，基于这个训练集估计一个regression tree，再用这个tree模型对剩下的色块分类。再比如做文本分析的时候，先雇人进行词语分类，再以此为标准让机器接下去做。类似的套路已经非常常见了。机器学习带来实质性改变的研究，我所知道的一个是哈佛大学穆纳莱森团队的工作，他们根据美国公民病历上显示的初始症状和最终的病情，预测最优的诊断方式，发现高风险的病人往往没有得到足够的诊断，而低风险的病人却进行了太多的诊断；还有一个是东北大学的政治学家做的，用推特文本来预测美国各个州的民调数据。想到的大概是这些，今后再补充……", "answer_votes": "79", "answer_comment": "​4 条评论"}
{"answer_author": "SHAN", "answer_id": 113753946, "answer_text": "作为从业人员且来灌个水，应该说Fintech是这两年很火的概念，在Fintech里很重要的一部分就是利用机器学习进行大数据风控----至少在你能看到的互联网金融企业里大半部分都是这么标榜的。所以我就从这个领域切入来回答下这个问题吧。其实把机器学习、大数据、风险控制这几个字组合在一起的时候，我是在发出低沉的笑：大家都只是想看到自己想看到的信息，不管它对与错，不管自己懂不懂。就像知乎点赞一样，我相对多的部分赞给了我完全不懂的天体物理，不懂，没法验证信息的真伪，只知道看起来很牛逼。互联网金融企业给公众的介绍的时候当然也有类似的效应：风控是金融企业的核心之一，大数据又是极其时髦的字眼，机器学习更是在AlphaGo之后为公众所熟知，结合在一起给投资者满满地信心。但拉句仇恨的说，这每一个词背后都是极其专业的学科，大部分投资人看得懂个屁！鄙人做了七年风控，但不敢说自己精于风控；做了两年数据，觉得还没入门！很多人是不是觉得大数据风控，就是在一堆海量数据的基础上，用上最时髦的算法，然后就可以把风控做好了。不排除很多同行招了一堆数据科学家，也在实验新的机器学习的算法。但是在绝大部分风控场景里，几乎没有领导会因为某个机器学习算法的AUC高就直接采用它。虽然GBDT和LR融合的效果很不错，但是我们的很大部分时间都在做特征工程。虽然我们自己私下里会去尝试着用SVM去做分类，很多大数据风控比赛也有用神经网络，但是在实际业务中我们还是会回到LR，不说部署的问题，把SVM的超平面距离转化成预测的概率就是个难题，ScikitLearn里的输出的Probability也是基于一篇实验性的论文而已，而传统的逻辑回归不但可以输出概率，而且可以进行刻度转换，得到每一个变量每一个分组对最后score的影响，从业务上说很好解释，也很好发现问题。如果上一段是在说机器学习算法发展和实际应用的不同步，那么下面就来说说机器学习本身的局限性。我不确定是否有一天银行可以做到机器学习来控制放款，但至少可以肯定，我的的工作暂时还不会全部被CS专业的人抢走，modelor光有算法还不够，风控的业务经验还能作为老本吃几年。举个例子，假设有个变量叫夜生活频繁程度，在统计量看，IV很高，而且表现出良好的单调性，用还是不用。我相信我的大部分同行在搞清楚它背后所有包含的信息之前是不会贸然让它进模型的。是这群人交际广业务繁忙？是这群人年轻喜欢夜夜笙歌？是这群人有很大部分特殊从业？和地区交叉看一看？和收入交叉看一看？在业务层面总会有很多的思考，而不是靠机器学习给出个结论说，这群人就是越多夜生活越bad。所以回到题主的问题，这个领域当然有得发展了，但是我的想法是，既然是作用于经济，那你要意识到经济的主体永远是人，人的行为人的决策人的动机，当我们用机器学习去拟合人的行为的时候，一定不要迷信算法，人是Bias，人是Variance。最后说句中性的话，前途是光明的，道路是漫长的。", "answer_votes": "55", "answer_comment": "​11 条评论"}
{"answer_author": "机器之心", "answer_id": 203454619, "answer_text": "用Python和机器学习炒股赚钱，想想是个挺诱人的应用瑞士日内瓦的一位金融数据顾问 Gaëtan Rickter 发表过一篇文章，介绍了他利用 Python 和机器学习来帮助炒股的经验，其最终成果的收益率跑赢了长期处于牛市的标准普尔 500 指数。虽然这篇文章并没有将他的方法完全彻底公开，但已公开的内容或许能给我们带来如何用人工智能炒股的启迪。机器之心对本文进行了编译介绍，代码详情请访问原文，地址在此。我终于跑赢了标准普尔 500 指数 10 个百分点！听起来可能不是很多，但是当我们处理的是大量流动性很高的资本时，对冲基金的利润就相当可观。更激进的做法还能得到更高的回报。这一切都始于我阅读了 Gur Huberman 的一篇题为《Contagious Speculation and a Cure for Cancer: A Non-Event that Made Stock Prices Soar》的论文。该研究描述了一件发生在 1998 年的涉及到一家上市公司 EntreMed（当时股票代码是 ENMD）的事件：「星期天《纽约时报》上发表的一篇关于癌症治疗新药开发潜力的文章导致 EntreMed 的股价从周五收盘时的 12.063 飙升至 85，在周一收盘时接近 52。在接下来的三周，它的收盘价都在 30 以上。这股投资热情也让其它生物科技股得到了溢价。但是，这个癌症研究方面的可能突破在至少五个月前就已经被 Nature 期刊和各种流行的报纸报道过了，其中甚至包括《泰晤士报》！因此，仅仅是热情的公众关注就能引发股价的持续上涨，即便实际上并没有出现真正的新信息。」在研究者给出的许多有见地的观察中，其中有一个总结很突出：「（股价）运动可能会集中于有一些共同之处的股票上，但这些共同之处不一定要是经济基础。」我就想，能不能基于通常所用的指标之外的其它指标来划分股票。我开始在数据库里面挖掘，几周之后我发现了一个，其包含了一个分数，描述了股票和元素周期表中的元素之间的「已知和隐藏关系」的强度。我有计算基因组学的背景，这让我想起了基因和它们的细胞信号网络之间的关系是如何地不为人所知。但是，当我们分析数据时，我们又会开始看到我们之前可能无法预测的新关系和相关性。选择出的涉及细胞可塑性、生长和分化的信号通路的基因的表达模式 和基因一样，股票也会受到一个巨型网络的影响，其中各个因素之间都有或强或弱的隐藏关系。其中一些影响和关系是可以预测的。我的一个目标是创建长的和短的股票聚类，我称之为「篮子聚类（basket clusters）」，我可以将其用于对冲或单纯地从中获利。这需要使用一个无监督机器学习方法来创建股票的聚类，从而使这些聚类之间有或强或弱的关系。这些聚类将会翻倍作为我的公司可以交易的股票的「篮子（basket）」。首先我下载了一个数据集：http://54.174.116.134/recommend/datasets/supercolumns-elements-08.html，这个数据集基于元素周期表中的元素和上市公司之间的关系。然后我使用了 Python 和一些常用的机器学习工具——scikit-learn、numpy、pandas、matplotlib 和 seaborn，我开始了解我正在处理的数据集的分布形状。为此我参考了一个题为《Principal Component Analysis with KMeans visuals》的 Kaggle Kernelimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition \nimport PCA\nfrom sklearn.cluster \nimport KMeans\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nnp.seterr(divide='ignore', invalid='ignore')\n# Quick way to test just a few column features\n\n# stocks = pd.read_csv('supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv', usecols=range(1,16))\n\nstocks = pd.read_csv('supercolumns-elements-nasdaq-nyse-otcbb-general-UPDATE-2017-03-01.csv')\n\nprint(stocks.head())\n\nstr_list = []\nfor colname, colvalue in stocks.iteritems():    \nif type(colvalue[1]) == str:\n         str_list.append(colname)\n# Get to the numeric columns by inversion\n\nnum_list = stocks.columns.difference(str_list)\n\nstocks_num = stocks[num_list]\n\nprint(stocks_num.head())\n输出：简单看看前面 5 行：概念特征的皮尔逊相关性（Pearson Correlation）。在这里案例中，是指来自元素周期表的矿物和元素：stocks_num = stocks_num.fillna(value=0, axis=1)\n\nX = stocks_num.values\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\n\nf, ax = plt.subplots(figsize=(12, 10))\nplt.title('Pearson Correlation of Concept Features (Elements & Minerals)')\n# Draw the heatmap using seaborn\n\nsb.heatmap(stocks_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap=\"YlGnBu\", linecolor='black', annot=True)\nsb.plt.show()\n输出：（这个可视化例子是在前 16 个样本上运行得到的）。看到元素周期表中的元素和上市公司关联起来真的很有意思。在某种程度时，我想使用这些数据基于公司与相关元素或材料的相关性来预测其可能做出的突破。测量「已解释方差（Explained Variance）」和主成分分析（PCA）已解释方差=总方差-残差方差（explained variance = total variance - residual variance)。应该值得关注的 PCA 投射组件的数量可以通过已解释方差度量（Explained Variance Measure）来引导。Sebastian Raschka 的关于 PCA 的文章对此进行了很好的描述，参阅：http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html# Calculating Eigenvectors and eigenvalues of Cov matirx\n\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n# Create a list of (eigenvalue, eigenvector) tuples\n\neig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n# Sort from high to low\n\neig_pairs.sort(key = lambda x: x[0], reverse= True)\n# Calculation of Explained Variance from the eigenvaluestot = sum(eig_vals)\nvar_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] \ncum_var_exp = np.cumsum(var_exp) \n# Cumulative explained variance# Variances plot\n\nmax_cols = len(stocks.columns) - 1plt.figure(figsize=(10, 5))\nplt.bar(range(max_cols), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\nplt.step(range(max_cols), cum_var_exp, where='mid',label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc='best')\nplt.show()\n输出：从这个图表中我们可以看到大量方差都来自于预测主成分的前 85%。这是个很高的数字，所以让我们从低端的开始，先只建模少数几个主成分。更多有关分析主成分合理数量的信息可参阅：http://setosa.io/ev/principal-component-analysis使用 scikit-learn 的 PCA 模块，让我们设 n_components = 9。代码的第二行调用了 fit_transform 方法，其可以使用标准化的电影数据 X_std 来拟合 PCA 模型并在该数据集上应用降维（dimensionality reduction）。pca = PCA(n_components=9)\nx_9d = pca.fit_transform(X_std)\n\nplt.figure(figsize = (9,7))\nplt.scatter(x_9d[:,0],x_9d[:,1], c='goldenrod',alpha=0.5)\nplt.ylim(-10,30)\nplt.show()\n输出：这里我们甚至没有真正观察到聚类的些微轮廓，所以我们很可能应该继续调节 n_component 的值直到我们得到我们想要的结果。这就是数据科学与艺术（data science and art）中的「艺术」部分。 现在，我们来试试 K-均值，看看我们能不能在下一章节可视化任何明显的聚类。K-均值聚类（K-Means Clustering）我们将使用 PCA 投射数据来实现一个简单的 K-均值。使用 scikit-learn 的 KMeans() 调用和 fit_predict 方法，我们可以计算聚类中心并为第一和第三个 PCA 投射预测聚类索引（以便了解我们是否可以观察到任何合适的聚类）。然后我们可以定义我们自己的配色方案并绘制散点图，代码如下所示：# Set a 3 KMeans clustering\n\nkmeans = KMeans(n_clusters=3)\n# Compute cluster centers and predict cluster indices\n\nX_clustered = kmeans.fit_predict(x_9d)# Define our own color map\n\nLABEL_COLOR_MAP = {0 : 'r',1 : 'g',2 : 'b'}\nlabel_color = [LABEL_COLOR_MAP[l] for l in X_clustered]\n# Plot the scatter digram\n\nplt.figure(figsize = (7,7))\nplt.scatter(x_9d[:,0],x_9d[:,2], c= label_color, alpha=0.5)\nplt.show()\n输出：这个 K-均值散点图看起来更有希望，好像我们简单的聚类模型假设就是正确的一样。我们可以通过这种颜色可视化方案观察到 3 个可区分开的聚类。当然，聚类和可视化数据集的方法还有很多，参考：https://goo.gl/kGy3ra使用 seaborn 方便的 pairplot 函数，我可以以成对的方式在数据框中自动绘制所有的特征。我们可以一个对一个地 pairplot 前面 3 个投射并可视化：# Create a temp dataframe from our PCA projection data \"x_9d\"\n\ndf = pd.DataFrame(x_9d)\ndf = df[[0,1,2]]\ndf['X_cluster'] = X_clustered\n# Call Seaborn's pairplot to visualize our KMeans clustering on the PCA projected data\n\nsb.pairplot(df, hue='X_cluster', palette='Dark2', diag_kind='kde', size=1.85)\nsb.plt.show()\n输出：构建篮子聚类（Basket Clusters）你应该自己决定如何微调你的聚类。这方面没有什么万灵药，具体的方法取决于你操作的环境。在这个案例中是由隐藏关系所定义的股票和金融市场。一旦你的聚类使你满意了，你就可以设置分数阈值来控制特定的股票是否有资格进入一个聚类，然后你可以为一个给定的聚类提取股票，将它们作为篮子进行交易或使用这些篮子作为信号。你可以使用这种方法做的事情很大程度就看你自己的创造力以及你在使用深度学习变体来进行优化的水平，从而基于聚类或数据点的概念优化每个聚类的回报，比如 short interest 或 short float（公开市场中的可用股份）。你可以注意到了这些聚类被用作篮子交易的方式一些有趣特征。有时候标准普尔和一般市场会存在差异。这可以提供本质上基于「信息套利（information arbitrage）」的套利机会。一些聚类则和谷歌搜索趋势相关。看到聚类和材料及它们的供应链相关确实很有意思，正如这篇文章说的一样：https://www.fairphone.com/en/2017/05/04/zooming-in-10-materials-and-their-supply-chains/我仅仅使用该数据集操作了 Cobalt（钴）、Copper（铜）、Gallium（镓）和 Graphene（石墨烯）这几个列标签，只是为了看我是否可能发现从事这一领域或受到这一领域的风险的上市公司之间是否有任何隐藏的联系。这些篮子和标准普尔的回报进行了比较。通过使用历史价格数据（可直接在 Quantopian、Numerai、Quandl 或 Yahoo Finance 使用），然后你可以汇总价格数据来生成预计收益，其可使用 HighCharts 进行可视化：我从该聚类中获得的回报超过了标准普尔相当一部分，这意味着你每年的收益可以比标准普尔还多 10%（标准普尔近一年来的涨幅为 16%）。我还见过更加激进的方法可以净挣超过 70%。现在我必须承认我还做了一些其它的事情，但因为我工作的本质，我必须将那些事情保持黑箱。但从我目前观察到的情况来看，至少围绕这种方法探索和包装新的量化模型可以证明是非常值得的，而其唯一的缺点是它是一种不同类型的信号，你可以将其输入其它系统的流程中。生成卖空篮子聚类（short basket clusters）可能比生成买空篮子聚类（long basket clusters）更有利可图。这种方法值得再写一篇文章，最好是在下一个黑天鹅事件之前。如果你使用机器学习，就可能在具有已知和隐藏关系的上市公司的寄生、共生和共情关系之上抢占先机，这是很有趣而且可以盈利的。最后，一个人的盈利能力似乎完全关乎他在生成这些类别的数据时想出特征标签（即概念（concept））的强大组合的能力。我在这类模型上的下一次迭代应该会包含一个用于自动生成特征组合或独特列表的单独算法。也许会基于近乎实时的事件，这可能会影响那些具有只有配备了无监督学习算法的人类才能预测的隐藏关系的股票组。文章发布于微信公众号：机器之心（almosthuman2014），如需转载，请私信联系，感谢。", "answer_votes": "100", "answer_comment": "​6 条评论"}
{"answer_author": "ET Tang", "answer_id": 75794870, "answer_text": "1. 引用身边一位经济学PhD对ML乃至现在广泛流行的data science的概括：\"data scientist is the modern fengshui master.\"我估计大多数严肃的经济学者会赞成这个观点。（但不屑归不屑，这并不妨碍他们学ML，毕竟业界喜欢简单暴力的prediction而非据说表现了casual inference的conceptional toys，这也反映出两者本质差别）2. 但两者却有交叉部分，这个部分恰恰不是在实证领域，如 Learning and Expectations in Macroeconomics (豆瓣)，尤其在最近几年，关于information friction的讨论，大量的宏观经济论文使用了social learning来取代rational expectation，认为经济主体是个“学习者”，某些具体的learning algorithm估计ML实践者们会有点熟悉。", "answer_votes": "76", "answer_comment": "​36 条评论"}
{"answer_author": "言施", "answer_id": 75273333, "answer_text": "目前好像还看不到前景。毕竟ML注重的是预测，而经济学要找的是因果关系，本身的出发点就不太一样。", "answer_votes": "4", "answer_comment": "​8 条评论"}
{"answer_author": "阿萨姆", "answer_id": 142596273, "answer_text": "作为一个在金融服务类公司从事AI开发工作的人，我想谈谈人工智能对于经济学可能有什么影响。当然，机器学习只是人工智能下的一个分支，虽然名字一直在变但都脱不了统计学习的外壳，自古有之。先上一个我不成熟的结论：经济学是一个复杂的理论学科，在短时间内，很难有AI大规模发挥的空间。从金融学角度来看，在利润率较高、数据结构化较好、问题定义明确的一些金融模型方面，AI会大行其道。不同于金融学领域更加偏向应用，经济学更加侧重理论，直接产生利润的机会较低，直接应用的机会较少，所以在利用AI上可能发展相对比较缓慢。然而机会在于，在金融领域使用AI探索数据的过程中，有很高的可能性发现系统性的规律或者违反规律的地方，从而反哺经济学。因此答主大胆的推测，大的金融公司在人工智能研究中可能发现/证实/证伪一些经济学规律。随着整个人工智能生态环境的逐步进步，机器学习或是其他人工智能手段，如自然语言处理，能够更好的服务于经济学发展。（以下部分内容节选自我在其他问题下的回答）答主觉得利用机器学习来为推动经济学发展，现阶段主要面临三个主要问题：1. 机器学习模型的可解释性低举个例子，机器学习在工业界最流行的模型就是logistic regression和random forest，是因为这两个模型的准确度/表现最好么？不是，因为这两个模型具有可解释性和可视化。对于管理者/监管者来说太重要了。同样，在经济学领域模型的可解释化也很重要，毕竟经验科学很难被当做理论来证明。现阶段的大部分ML模型都面临效果不错解释不得的问题。越来越多的论文在尝试提高模型的可解释性，比如 \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier，就尝试证明了通过通用手段来证明机器学习模型的正确性。仅当这个领域继续发展以后，我们才不单单把机器学习当做应用，而是探索理论发展的工具。2. 机器学习模型的定义难现阶段比较被商业化广泛应用的机器学习还是监督学习，而监督学习要求有明确的问题定义。现在看起来很有希望的强化学习，迁移学习等还并不能大规模普及应用。以简单的监督学习为例，如果你想建立一个模型来预测企业并购是否会影响公司股价，那么你需要提供大量并购数据，以及并购后股价是否发生了变动。理想情况下，在收集足够多的并购消息和股价变动信息后，做自然语言分析后提取特征放到机器学习模型里面就大功告成了。然而在实际情况中：我们无法给出明确的问题定义和边界。如果想用AI来来制定一个股票交易策略，那么需要考虑进去多少因素？仅仅只考虑并购消息就够了么？越多的相关的因素越可以提高模型的拟合性和准确性。如宏观政策和微观的具体情况都会影响到股价的波动，漏掉其中哪一个都会造成一定的影响，往往是多多益善。在这种情况下，每个问题都需要大量人和数据来支撑，这也是为什么大量用AI来预测股票走势的探索都无疾而终的原因。现阶段或者可预见的未来，在很多经济学问题上不大可能出现这种明确的定义和范围。3. 数据的结构化程度差机器学习模需要结构化的数据，至少是电子数据。金融领域的大数据化，甚至是数据结构化都还有很长的路的要走。以审计为例，很多公司还有大量的票据都不能无纸化，更不要提AI能够消化的电子数据了。前一阵子我司开发一个面试AI，但是并没有原始数据可以直接使用。于是我们让12个刚入职的员工花了一周时间把我们保留的面试视频逐字逐句的转译到文字+特征，整个过程苦不堪言。因此利用机器学习来进行经济学研究，获得干净的结构化数据难度是很高的。作为一门理论基础学科，缺乏足够多的科研经费会成为购买数据的阻碍，而往往手握这些数据的大金融公司是不会随便出借/销售宝贵的数据，毕竟这是个数据为王的时代:)--------------------------------------------------------------------------------------更多有关人工智能和金融、经济学之间的分析，可以移步......金融学如何应对人工智能和大数据？ - 知乎用户的回答 - 知乎随着人工智能的进步，财务工作者会大批失业么？该如何应对？ - 金融 - 知乎", "answer_votes": "2", "answer_comment": "​1 条评论"}
{"answer_author": "黄一飞", "answer_id": 129823192, "answer_text": "机器学习和人工智能（简称ML/AI）将对经济学产生深远影响。ML/AI在实际应用中的成功，会倒逼经济学在工程性上与时俱进。商学院在这点上走在了经济系的前面。很大程度上是因为商学院的评价体系和价值取向更容易鼓励实际应用问题导向的研究。金融、会计领域的顶级期刊上已经有很多利用文本挖掘、自然语言处理、语音识别等技术来从文本、音频等非结构化数据中提取信息，来预测上市公司财务舞弊、预测股价的研究。当然，这些发表在顶级期刊上的论文自然也是能够联系到经济理论、金融会计理论的基本问题上来的（例如资本市场有效性、媒体对市场的影响、会计披露对市场的影响等等）。文献综述可以看这两篇：Textual Analysis in Accounting and Finance: A Survey (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2504147) 以及 Speech Analysis in Financial Markets (http://www.nowpublishers.com/article/Details/ACC-024).当真正有重要的应用问题需要解决的时候，才更容易催生务实的价值取向：不管黑猫白猫，逮到耗子才是好猫。搜索引擎关键词广告竞价拍卖的巨大商业价值催生了一批非常高质量的交叉学科研究和学术社区。ACM Economics and Computation是非常有声誉的顶级学术会议。举一个发表在ACM EC的具体的研究：Econometrics for Learning Agents (http://sticerd.lse.ac.uk/seminarpapers/em11122015.pdf). 这篇论文研究的对象是一个非常经典的拍卖实证问题：如何从卖拍的报价数据推断竞拍者对拍卖标的的真实估值。但是作者并不依赖拍卖结果是纳什均衡的假设，而假设竞拍者是不断地在学习最好的竞拍策略。这对于搜索引擎关键词竞价拍卖这个高频率并且拍卖利益巨大应用场景是很贴切的。对于“机器学习不讲因果关系只会肤浅地预测”的刻板印象，我建议研读一下2013年Journal of Machine Learning Research上的论文：Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising (http://jmlr.org/papers/volume14/bottou13a/bottou13a.pdf). 因果问题对于一个实际的机器学习系统非常重要。先挖个坑，等赞到100再补充。", "answer_votes": "62", "answer_comment": "​6 条评论"}
{"answer_author": "知乎用户", "answer_id": 75300937, "answer_text": "Summer Institute 2015 Methods Lectures这是NBER 今年夏天的一个讲座，主要是斯坦福的Susan Athey和Guido Imbens搞的，专门给经济学家介绍ML的", "answer_votes": "7", "answer_comment": "​添加评论"}
{"answer_author": "赵晓航", "answer_id": 76222623, "answer_text": "先放一篇博文，算是基本回答了题主的问题。http://www.bloombergview.com/articles/2015-09-01/economics-has-a-math-problem主要应用领域在于计量经济学。另外，除了前面推荐的15年的讲座，再推荐个13年的。Summer Institute 2013 Econometric Methods for High-Dimensional Data演讲者Matthew Taddy是芝加哥大学的教授。这个讲座没有直接回答你的问题，它关注的都是具体的技术。不过对于只有计量背景的人来说还是很开脑洞的。计量模型里通常感兴趣的只有一两个变量。其他大量的变量都是起控制作用。但是最终确定哪些变量有趣时，总要有个预选择的过程，尤其是你有大量的备选变量且没有现成的经济理论可供指导时。所以ML提供了一个很好的选择变量的方案（选择带有pattern的变量）。在此基础上，可以为这些观测到的pattern寻求更严格的因果解释。另外，推荐楼主翻翻欧美各大名校的计量课程的syllabus（各个教授的）。有些学校已经将regression tree作为计量课的一个章节讲授了。其实回答这个问题的最好方法，是遍历一遍顶级的经济学期刊，筛选出其中与machine learning有关的问题。本人才疏学浅，这方面的阅读经验是零。期待有相关经验的大神能给科普下。", "answer_votes": "3", "answer_comment": "​1 条评论"}
{"answer_author": "西瓜", "answer_id": 113791493, "answer_text": "有意向奔向此坑。过来先把flag立好。三年到五年后回来看。", "answer_votes": "1", "answer_comment": "​8 条评论"}
{"answer_author": "Julia ZHU", "answer_id": 78853862, "answer_text": "我已经决定奔向此坑了，就这个。我也嚣张一把，就让这FLAG在这高高挂起。经济学之其实很mei友di好xian准则：1.只要假设允许，方法论随意使用。2.反向使用统计潜规则，随意证明或证伪任何经济学理论。ps1.机器学习可以真真正正站在工具理性的角度用来迭代大规模的博弈模型啊！你们难道不知道这意味着什么吗？2.想来“解释变量”和“被解释变量”的名字还是优于“自变量”和“因变量”：因果和逻辑未必就有关。3.我还小，统计学并不敢信多少，节操还是想留给概率论。4.请用粗糙的chaos大胆蹂躏光滑的经济模型世界。5.看涨看空，没有投入其实没有意义。", "answer_votes": "2", "answer_comment": "​4 条评论"}
{"answer_author": "知乎用户", "answer_id": 78965254, "answer_text": "哈？没有么？讲真，我一直分不清机器学习和演化博弈论。", "answer_votes": "1", "answer_comment": "​3 条评论"}
{"answer_author": "Nebula.Trek", "answer_id": 117269797, "answer_text": "机器学习模拟的是人脑思考方式，是可以连续模拟计算几百万年的人脑。行业并非机器学习的方向。围棋程序和跳舞，是一个性质。", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "LCHEN", "answer_id": 105345436, "answer_text": "Varian 6月21日下午将会来我们院作个小范围的报告，题目是 Tools for Google Data。先占个坑，到时候回来写个总结报告。", "answer_votes": "5", "answer_comment": "​2 条评论"}
{"answer_author": "Dennis Wang", "answer_id": 75904101, "answer_text": "\"经济学都是用统计，要看因果关系什么的\"看到有人提到这个，只想提一句经济学的东西基本都是observational data，一般统计根本得不到因果关系，少说也要graphical model :)其实graphical model应该对经济学很有用，不管是Markov random field 还是Bayesian network 。看各位经济学家的努力了。至于其他model，有的的确能得出好结果，但从经济学的角度来看，根本无法解释中间的逻辑。", "answer_votes": "2", "answer_comment": "​1 条评论"}
{"answer_author": "知乎用户", "answer_id": 92855006, "answer_text": "有，我们这儿现在就有个机器学习用于经济相关的项目在进行。以前我听微软的人作报告，也有讲到他们用深度学习来自动炒股的。不过在天朝我是不看好用DNN来预测股市之类的。比如房地产市场，我觉得我的预测肯定比DNN要准。这点自信我还是有的。", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "知乎用户", "answer_id": 105657245, "answer_text": "别的不知道，至少用行业数据分析宏观态势很有效果", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "万一子", "answer_id": 75274639, "answer_text": "个人观点：目前没有。因为机器学习说白了只是试错比人快点，但依赖的框架是在人的理解基础之上的，而经济学对于人而言都不能算明白吧，对于很多经济现象的解释还处于事后诸葛亮阶段，而大数据本身是不问因果，只论相关的，所以对于解释现象其实并无帮助。所以在人工智能没有超越人类之前或者人类自身对于经济的理解产生重大突破之前机器学习几乎不可能有实际用途顺便一说：回答里什么量化，西蒙斯之类完全是答非所问，乱说一气，连经济和金融都没分清了", "answer_votes": "7", "answer_comment": "​1 条评论"}
{"answer_author": "知乎用户", "answer_id": 90078707, "answer_text": "假如你的目标是发学术论文求取功名的话，有假如你的目标是学以致用的话，至少目前看来，没有。今后会不会有，不知道。仅以现在来说，机器学习是数学家的事，不关经济学什么事。数学是工具，本身无法推出任何经济学结论。就像烧饭得有锅，但有个锅不代表你就会烧饭了。机器学习是一个好锅子，但你会不会烧饭，是不是非要机器学习这个锅子，那是另一回事。目前的机器学习得出的结论，准确率还不及直接用肉眼看。只有当变数过多，用任何方式都看不出结论，万不得已时才用，也就是我连蒙一个答案都蒙不对，那就用机器学习，跟瞎蒙比起来机器学习还是稍微准一点的。（然而考30分和考10分，都是不及格）", "answer_votes": "2", "answer_comment": "​1 条评论"}
{"answer_author": "裴小浩", "answer_id": 75168134, "answer_text": "西蒙斯与文艺复兴科技公司 2005年，西蒙斯成为全球收入最高的对冲基金经理，净赚15亿美元，差不多是索罗斯的两倍；从1988年开始，他所掌管的大奖章基金年均回报率高达34％，15年来资产从未减少过。　　西蒙斯几乎从不雇用华尔街的分析师，他的文艺复兴科技公司里坐满了数学和自然科学的博士。用数学模型捕捉市场机会，由电脑作出交易决策，是这位超级投资者成功的秘诀。　　“人们一直都在问我，你赚钱的秘密是什么？”几乎每次接受记者采访时，詹姆斯西蒙斯(James Simons)总会说到这句话，他似乎已经习惯了那些渴望的眼神。事实上，在对冲基金的世界里，那应该是每个人都想要了解的秘密。　　68岁的西蒙斯满头银发，喜欢穿颜色雅致的衬衫，光脚随意地蹬一双loafers牌休闲鞋。虽然已经成为《机构投资者》杂志年度最赚钱的基金经理，但还是有很多人不知道他到底是谁。西蒙斯曾经和华裔科学家陈省身共同创立了著名的Chern-Simons定律，也曾经获得过全美数学界的最高荣誉。在充满了传奇色彩的华尔街，西蒙斯和他的文艺复兴科技公司（Renaissance Technologies Corp.）是一个彻底的异类。　　作风低调的西蒙斯很少接受采访，不过自从他放弃了在数学界如日中天的事业转而开办投资管理公司后，二十多年间，西蒙斯已经创造了很多难以企及的记录，无论从总利润还是净利润计算，他都是这个地球上最伟大的对冲基金经理之一。　　以下是一些和西蒙斯有关的数字：1988年以来，西蒙斯掌管的的大奖章（Medallion）对冲基金年均回报率高达34%，这个数字较索罗斯等投资大师同期的年均回报率要高出10个百分点，较同期标准普尔500指数的年均回报率则高出20多个百分点；从2002年底至2005年底，规模为50亿美元的大奖章基金已经为投资者支付了60多亿美元的回报。　　这个回报率是在扣除了5％的资产管理费和44％的投资收益分成以后得出的，并且已经经过了审计。值得一提的是，西蒙斯收取的这两项费用应该是对冲基金界最高的，相当于平均收费标准的两倍以上。高额回报和高额收费使西蒙斯很快成为超级富豪，在《福布斯》杂志2006年9月发布的“400位最富有的美国人”排行榜中，西蒙斯以40亿美元的身家跻身第64位。　　模型先生　　针对不同市场设计数量化的投资管理模型，并以电脑运算为主导，在全球各种市场上进行短线交易是西蒙斯的成功秘诀。不过西蒙斯对交易细节一直守口如瓶，除了公司的200多名员工之外，没有人能够得到他们操作的任何线索。　　对于数量分析型对冲基金而言，交易行为更多是基于电脑对价格走势的分析，而非人的主观判断。文艺复兴公司主要由3个部分组成，即电脑和系统专家，研究人员以及交易人员。西蒙斯亲自设计了最初的数学模型，他同时雇用了超过70位拥有数学、物理学或统计学博士头衔的人。西蒙斯每周都要和研究团队见一次面，和他们共同探讨交易细节以及如何使交易策略更加完善。　　作为一位数学家，西蒙斯知道靠幸运成功只有二分之一的概率，要战胜市场必须以周密而准确的计算为基础。大奖章基金的数学模型主要通过对历史数据的统计，找出金融产品价格、宏观经济、市场指标、技术指标等各种指标间变化的数学关系，发现市场目前存在的微小获利机会，并通过杠杆比率进行快速而大规模的交易获利。目前市场上也有一些基金采取了相同的策略，不过和西蒙斯的成就相比，他们往往显得黯然失色。　　文艺复兴科技公司的旗舰产品——大奖章基金成立于1988年3月，到1993年，基金规模达到2.7亿美元时开始停止接受新资金。现在大奖章基金的投资组合包含了全球上千种股市以及其他市场的投资标的，模型对国债、期货、货币、股票等主要投资标的的价格进行不间断的监控，并作出买入或卖出的指令。　　当指令下达后，20名交易员会通过数千次快速的日内短线交易来捕捉稍纵即逝的机会，交易量之大甚至有时能占到整个纳斯达克市场交易量的10%。不过，当市场处于极端波动等特殊时刻，交易会切换到手工状态。　　和流行的“买入并长期持有”的投资理念截然相反，西蒙斯认为市场的异常状态通常都是微小而且短暂的，“我们随时都在买入卖出卖出和买入，我们依靠活跃赚钱”，西蒙斯说。　　西蒙斯透露，公司对交易品种的选择有三个标准：即公开交易品种、流动性高，同时符合模型设置的某些要求。他表示，“我是模型先生，不想进行基本面分析，模型的优势之一是可以降低风险。而依靠个人判断选股，你可能一夜暴富，也可能在第二天又输得精光。”　　西蒙斯的所作所为似乎正在超越有效市场假说：有效市场假说认为市场价格波动是随机的，交易者不可能持续从市场中获利。而西蒙斯则强调，“有些交易模式并非随机，而是有迹可循、具有预测效果的。”如同巴菲特曾经指出“市场在多数情况下是有效的，但不是绝对的”一样，西蒙斯也认为，虽然整体而言，市场是有效的，但仍存在短暂的或局部的市场无效性，可以提供交易机会。　　在接受《纽约时报》采访时，西蒙斯提到了他曾经观察过的一个核子加速器试验，“当两个高速运行的原子剧烈碰撞后，会迸射出数量巨大的粒子。”他说，“科学家的工作就是分析碰撞所带来的变化。”　　“我注视着电脑屏幕上粒子碰撞后形成的轨迹图，它们看似杂乱无章，实际上却存在着内在的规律，”西蒙斯说，“这让我自然而然地联想到了证券市场，那些很小的交易，哪怕是只有100股的交易，都会对这个庞大的市场产生影响，而每天都会有成千上万这样的交易发生。”西蒙斯认为，自己所做的，就是分析当交易这只蝴蝶的翅膀轻颤之后，市场会作出怎样复杂的反应。　　“这个课题对于世界而言也许并不重要，不过研究市场运转的动力非常有趣。这是一个非常严肃的问题。”西蒙斯笑起来的时候简直就像一个顽童，而他的故事，听起来更像是一位精通数学的书生，通过复杂的赔率和概率计算，最终打败了赌场的神话。这位前美国国防部代码破译员和数学家似乎相信，对于如何走在曲线前面，应该存在一个简单的公式，而发现这个公式则无异于拿到了通往财富之门的入场券。　　黑箱操作　　对冲基金行业一直拥有 “黑箱作业”式的投资模式，可以不必向投资者披露其交易细节。而在一流的对冲基金投资人之中，西蒙斯先生的那只箱子据说是“最黑的”。　　就连优秀的数量型对冲基金经理也无法弄清西蒙斯的模型究竟动用了哪些指标，“我们信任他，相信他能够在股市的惊涛骇浪中游刃有余，因此也就不再去想电脑都会干些什么之类的问题”，一位大奖章基金的长期投资者说。当这位投资者开始描述西蒙斯的投资方法时，他坦承，自己完全是猜测的。　　不过，每当有人暗示西蒙斯的基金缺乏透明度时，他总是会无可奈何地耸耸肩，“其实所有人都有一个黑箱，我们把他称为大脑。” 西蒙斯指出，公司的投资方法其实并不神秘，很多时候都是可以通过特定的方式来解决的。当然，他不得不补充说，“对我们来说，这其实不太神秘。”　　在纽约，有一句名言是：你必须非主流才能入流（You have to be out to be in），西蒙斯的经历似乎刚好是这句话的注解。在华尔街，他的所做所为总是让人感到好奇。　　西蒙斯的文艺复兴科技公司总部位于纽约长岛，那座木头和玻璃结构的一层建筑从外表看上去更像是一个普通的脑库，或者是数学研究所。和很多基金公司不同的是，文艺复兴公司的心脏地带并不是夜以继日不停交易的交易室，而是一间有100个座位的礼堂。每隔半个月，公司员工都会在那里听一场科学演讲。“有趣而且实用的统计学演讲，对你的思想一定会有所启发。”一位喜欢这种学习方式的员工说。　　令人惊讶的还不止这些。西蒙斯一点也不喜欢华尔街的投资家们，事实上，如果你想去文艺复兴科技公司工作的话，华尔街经验反而是个瑕疵。在公司的200多名员工中，将近二分之一都是数学、物理学、统计学等领域顶尖的科学家，所有雇员中只有两位是金融学博士，而且公司从不雇用商学院毕业生，也不雇用华尔街人士，这在美国的投资公司中堪称绝无仅有。　　“我们不雇用数理逻辑不好的学生”，曾经在哈佛大学任教的西蒙斯说。“好的数学家需要直觉，对很多事情的发展总是有很强的好奇心，这对于战胜市场非常重要。”文艺复兴科技公司拥有一流的科学家，其中包括贝尔试验室的著名科学家Peter Weinberger和弗吉尼亚大学教授Robert Lourie。他还从IBM公司招募了部分熟悉语音识别系统的员工。“交易员和语音识别的工作人员有相似之处，他们总是在猜测下一刻会发生什么。”　　人员流动几乎是不存在的。每6个月，公司员工会根据业绩收到相应的现金红利。据说半年内的业绩基准是12%，很多时候这个指标可以轻松达到，不少员工还拥有公司的股权。西蒙斯很重视公司的气氛，据说他经常会和员工及其家属们分享周末，早在2000年，他们就曾一起飞去百慕大度假。与此同时，每一位员工都发誓要保守公司秘密。　　近年来，西蒙斯接受最多的质疑都与美国长期资本管理公司(LTCM)有关。LTCM在上世纪90年代中期曾经辉煌一时，公司拥有两位诺贝尔经济学奖得主，他们利用计算机处理大量历史数据，通过精密计算得到两个不同金融工具间的正常历史价格差，然后结合市场信息分析它们之间的最新价格差。如果两者出现偏差，电脑立即发出指令大举入市；经过市场一段时间调节，放大的偏差会自动恢复到正常轨迹上，此时电脑指令平仓离场，获取偏差的差值。　　LTCM始终遵循“市场中性”原则，即不从事任何单方面交易，仅以寻找市场或商品间效率落差而形成的套利空间为主，通过对冲机制规避风险，使市场风险最小。但由于其模型假设前提和计算结果都是在历史统计数据基础上得出的，一旦出现与计算结果相反的走势，则对冲就变成了一种高风险的交易策略。　　而在极大的杠杆借贷下，这种风险被进一步放大。最辉煌时，LTCM利用从投资者筹得的22亿美元资本作抵押，买入价值1250亿美元证券，然后再以证券作为抵押，进行总值12500亿美元的其他金融交易，杠杆比率高达568倍。短短4年中，LTCM曾经获得了285%的收益率，然而，在过度操纵之下，又在仅两个月之内又输掉了45亿美元，走向了万劫不复之地。　　“我们的方式和LTCM完全不同”，西蒙斯强调，文艺复兴科技公司没有、也不需要那么高的杠杆比例，公司在操作时从来没有任何先入为主的概念，而是只寻找那些可以复制的微小的获利瞬间，“我们绝不以‘市场恢复正常’作为赌注投入资金，有一天市场终于会正常的，但谁知道是哪一天。”　　西蒙斯的拥护者们也多半对黑箱操作的风险不以为然，他们说，“长期资本公司只有两位诺贝尔奖金获得者充当门面，主要的还是华尔街人士，他们的赌性决定了终究会出错”，另一位著名的数量型基金管理人也表示，“难以相信在西蒙斯的方法中会没有一些安全措施。” 他指出，西蒙斯的方法和LTCM最重要的区别是不涉及对冲，而多是进行短线方向性预测，依靠同时交易很多品种、在短期作出大量的交易来获利。具体到每一个交易的亏损，由于会在很短的时间内平仓，因此损失不会很大；而数千次交易之后，只要盈利交易多余亏损交易，总体交易结果就是盈利的。　　数学大师　　西蒙斯很少在金融论坛上发表演讲，他喜欢的是数学会议，他在一个几何学研讨会上庆祝自己的60岁生日，为数学界和患有孤独症的儿童捐钱，在发表演讲时，更常常强调是数学使他走上了投资的成功之路。有人说，和华尔街的时尚毫不沾边或许也是他并不瞩目的原因之一。　　西蒙斯在数学方面有着天生的敏感和直觉，这个制鞋厂老板的儿子3岁就立志成为数学家。高中毕业后，他顺利地进入了麻省理工学院，大学毕业仅三年，就拿到了加州大学伯克利分校的博士学位，24岁就出任哈佛大学数学系教授。　　不过，尽管已经是国际数学界的后起之秀，他还是很快就厌倦了学术生涯。1964年，天生喜欢冒险的西蒙斯进入美国国防部下属的一个非盈利组织——国防逻辑分析协会进行代码破解工作。后来由于反对越战，他又重回学术界，成为纽约州立石溪大学(Stony Brook University)的数学系主任，在那里做了8年的纯数学研究。　　西蒙斯很早以前就曾和投资结缘，1961年，他和麻省理工学院的同学投资于哥伦比亚地砖和管线公司；在伯克利时也曾投资一家婚礼礼品的公司，但结果都不太理想，当时他觉得股市令人烦恼的，“我还曾经找到美林公司的经纪人，试图做些大豆交易”，西蒙斯说。　　直到上世纪70年代早期，西蒙斯才开始真正对投资着迷。那时他还在石溪大学任教，他身边的一位数学家参与了一家瓷砖公司出售的交易，“8个月的时间里赚了我10倍的钱。”　　70年代末，当他离开石溪大学创立私人投资基金时，最初也采用基本面分析的方式，“我没有想到用科学的方法进行投资，”西蒙斯说，那一段时间他主要投资于外汇市场，“随着经验的不断增加我想到也许可以用一些方法来制作模型，预见货币市场的走势变动。”　　80年代后期，西蒙斯和普林斯顿大学的数学家勒费尔(Henry Larufer)重新开发了交易策略，并从基本面分析转向数量分析。从此，西蒙斯彻底转型为“模型先生”，并为大奖章基金接近500位投资人创造出了令人惊叹的业绩。　　2005年，西蒙斯宣布要成立一只规模可能高达1000亿美元的新基金，在华尔街轰动一时，要知道，这个数字几乎相当于全球对冲基金管理资产总额的十分之一。谈到新基金时，西蒙斯更加谨慎，他表示，和大奖章基金主要针对富有阶层不同，新基金的最低投资额为2000万美元，主要面向机构投资者，将通过下调收费来吸引投资；此外，新基金将偏重于投资美国股市，持有头寸超过一年——相对于大奖章的快速交易而言，新基金似乎开始坚持“买入并持有”的理念。　　“对大奖章非常有效的模型和方法并不一定适用于新基金”，看来西蒙斯相信，对于一个金额高达千亿的对冲基金来说，如果还采用类似于大奖章的操作方法的话，一定是非常冒险的。　　尽管新基金有着良好的血统，不过不少投资者仍然怀疑它究竟能有多大的作为，一个起码的事实是，相对于一些流动性差的小型市场而言，高达1000亿美元的基金规模可能显得太大，这将增加它们在退出时的困难。　　尽管怀疑的声音很多，到2006年2月中旬，西蒙斯还是筹集到了40亿美金，并表示将吸收更多的资金。公司同时向投资者承诺，一旦在任何时点基金运作出现疲弱的迹象，就将停止吸收新资金，届时新基金将不再继续增加到千亿美金的上限。 ", "answer_votes": "16", "answer_comment": "​4 条评论"}
{"answer_author": "岂安科技", "answer_id": 211388857, "answer_text": "这里我有一个问题想要提出，那就是机器学习最重要的三样东西。以下，GO~如果我们从机器学习取得不错效果的领域去窥见的话，也许能找到一些思路：无论是语音识别、图片识别、还是 AlphaGO 的应用都有现成的大量的音频、图片、棋谱的样本去训练，而商品推荐、音乐推荐系统这些相对低频的累计用户的正反馈的方式无疑例外的都指出了机器学习的正途无疑是积累样本。<img src=\"https://pic3.zhimg.com/50/v2-cff19202aff6d83b63de81ef0e650aaa_hd.jpg\" data-rawwidth=\"580\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb\" width=\"580\" data-original=\"https://pic3.zhimg.com/v2-cff19202aff6d83b63de81ef0e650aaa_r.jpg\">虽然近年来又有类似 GAN 对抗网络这种非监督式机器学习的兴起，但是也鲜闻有特别成功的商业应用。我们这时总结到: 机器学习重要的东西有三样：样本, 样本, 还是样本。《Outside the Closed World:On Using Machine Learning For Network Intrusion Detection》该论文比较详细的分析了机器学习为什么没有在安全监测方面取得广泛应用的原因：机器学习是寻找相似性，没有足够多的恶意访问、攻击的样本，无从训练。分类错误的成本很高，且一般实际应用的良好准确率的模型都是 *黑盒* ， *无法解释* 、*无法人工干预* 。网络监控到实际的检测对象之间的抽象成本，即比如说一个撞库活动与底层网络信息通讯之间的对应关系有一个抽象、翻译的难度。所以坊间甚嚣尘上的机器学习的传闻是多么美好，我们凑近一看，其实还是镜花水月，所以大部分愿意使用机器学习的公司基本都是拿机器学习的结果当做参考，最终还是要靠人的判断。", "answer_votes": "11", "answer_comment": "​1 条评论"}
{"answer_author": "知乎用户", "answer_id": 79131226, "answer_text": "排名第一的答案说到计量里的工具变量选择问题，前几天刚好在学校听了老师的一个非常有意思的讲座，题目是：Nonparametric Additive Instrumental Variable\nEstimator: A Group Shrinkage Estimation Perspective ，即NAIVE，使用了GROUP LASSO和非参可加模型，应该可以算是统计学习在工具变量选择中的应用吧。相应的论文在这里：https://editorialexpress.com/cgi-bin/conference/download.cgi?db_name=ESWC2015&paper_id=241", "answer_votes": "1", "answer_comment": "​1 条评论"}
{"answer_author": "上大飞猪钱小莲", "answer_id": 74403072, "answer_text": "有啊，机器学习模型大多用于分类和预测，经济学中的个体行为研究和经济变量预测都可以实用。我还看到在传统的egarch模型中引入回归树等应用方法的。", "answer_votes": "2", "answer_comment": "​添加评论"}
{"answer_author": "GarryHe", "answer_id": 80664950, "answer_text": "阅读了上面的回答 很多人对ML的理解非常的深刻到位 在此我就不鲁班门前了. 但是大家对基本上都忽略了经济学理论这个问题. 以我个人的意见 二者之间可以互相帮助知识的学习 但是运用的空间很小. 回答这个问题之前 需要把一些概念说清楚. 首先经济学和ML唯一的交集 就是econometrics. 也就是说 只有在量化的经济学领域 才用的上与ML有关的知识. 而ML又是一个很广的概念 其中能运用到经济学上的无非就是regression类. 然后来说一下regression 无非就是y=bx+e. 而经济学的regression 有两个非常重要的要求：1, x必须为显著(一般取95%) 2, 每个model背后必须有经济学理论作为支持 否则就算regression是显著的 也不能称之为有效的. regression的运用非常广 从宏观到微观 凡是可量化的参数 找到有效的data 都可以运用regression. 而如今经济学唯一的出路就是量化 否则一门社会科学在如今的大数据时代是无法证明自己的理论的. 而ML注重的是降纬 例如当你的input data有300纬度的时候 你不可能做一个dim(x)=300的回归 你只能选择降纬. 简单的方法有ridge lasso pca等. 而ML做出来的东西 既不要求x显著 也不需要有任何理论支持.所以结论就是 当你运用了ML 的methodology在一个经济学数据上的时候 并没有ML所带来的好处(一般的经济学model不需要降纬 一般情况下本来x也就不多) 而所得出来的结论就算在predictive power上面很强 但是并没有一个理论在后面支持 这样的model在经济学领域并不能说服他人.所以我的意见是 如果你想成为一个经济学家 并不需要很复杂的ML背景 只要学好统计学好计量就可以了, 而如果你是想研究ML的话 经济学并不是最理想的应用领域, ML一般是在金融领域运用的比较广泛 例如现在的hedge fund. 而在医药学 生物学等上 也有很广泛的运用. 但是在经济学上的应用前景 我觉得有待商榷. ", "answer_votes": "2", "answer_comment": "​添加评论"}
{"answer_author": "Holzki", "answer_id": 76089942, "answer_text": "Varian写过篇文章，建议经济学家学习一下data science/machine learning的工具http://people.ischool.berkeley.edu/~hal/Papers/2013/ml.pdf感觉可以参考一下", "answer_votes": "2", "answer_comment": "​添加评论"}
{"answer_author": "hhhhh", "answer_id": 285724752, "answer_text": "前两天在厦大举办的计量经济学会以上，Whitney Newey教授在题为“Machine Learning of Structural Parameters”的报告中，提到有许多经济参数都依赖于未知函数，而未知函数可能是高维度的，例如较大的数据库或是众多价格，而机器学习提供了灵活、可计算的函数估计的方法，由此可以应用于估计结构参数。随后Newey教授介绍了其如何将有关结构参数机器学习的研究应用到实例当中，其一，用于在已给出的众多商品价格的数据条件下进行需求分析时估计平均剩余的边界值、税收对不同收入群体的影响等。其二，用于对动态离散选择的机器学习研究。", "answer_votes": "1", "answer_comment": "​添加评论"}
{"answer_author": "知乎用户", "answer_id": 146925707, "answer_text": "窃以为machine learning在经济学领域更重要的是思维方式", "answer_votes": "1", "answer_comment": "​添加评论"}
{"answer_author": "henryWang", "answer_id": 75135729, "answer_text": "机器学习处理的是面对数据做什么决策的问题，不是针对某个行业呀。卖菜的也可以耍ML啊", "answer_votes": "1", "answer_comment": "​添加评论"}
{"answer_author": "宋爷", "answer_id": 190201346, "answer_text": "看一下高盛北美的交易员之前有多少，现在还剩多少，然后是因为什么人数上有有差别的，你就知道机器学习在经济学领域是否有应用场景了  (￣▽￣)推荐看一下TensorFlow这个机器学习系统能用来做什么的视频： 「TensorFlow第一回」TensorFlow能用来做什么？", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "XG Liu", "answer_id": 159459433, "answer_text": "机器学习核心在于y hat，过程中间采用了cross validation来确保整个拟合结果的稳健，要求并不过拟合overfit，也不欠拟合underfit。实战中我们看到机器学习大量应用于日常生活，比如简单的垃圾邮件自动判断，复杂的alphago就是能赢世界冠军李世石九段，y hat就是猛。经济学中的回归方程要想出好的机制来精准识别x对y的因果关系beta hat。如果识别干净确实很漂亮，而且y hat也就能顺利计算出。但实战过程中呢？传统经济学家把玩的数据量太小，beta hat识别干净好像在生活中贡献也不大。", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "老钱", "answer_id": 150325767, "answer_text": " 一句话总结：机器可以打败市场。\n 投资市场中的亏损者画像是这样的：自认为聪明的头脑＋饥饿的胃口＝大概率亏损\n老钱自己是智能投顾方向的从业者，所以，在投资方面一向更信任机器和算法。凡是能拼算法的地方，一般人都干不过机器。\n今天通俗易懂地聊聊量化基金。1.用人话解释量化基金就是，通过统计分析，选出评分最高的股票组合。\n那么，股票评分的标准何来呢？通过多因子模型，一个因子，就是一个判断维度。\n基金经理通过计算机和算法，从多个维度对股票进行打分，最终选出评分最高的。\n判断股票的因子有很多，一支量化基金的因子库里可能有几十上百个因子。比如最基本的：估值、成长、财务、量价、风险、均线金叉、持续放量、游资进场、并购重组、股权激励、定向增发、高送转等等...\n但量化基金不可能同时把所有因子都考虑进去。市场在不同的行情下，都有最适合当下的策略。量化基金选股的核心就在于，构建出适合当前市场的多因子选股模型。\n构建模型有两个原则：\n选择当下最能获得超额收益的因子选择能够有效控制风险的因子模型一旦确认，评分标准就有了。\n接下来，会用机器和算法，对市场上每一支股票按照标准进行打分，排序。\n最后，由基金经理用自己的标准和经验，挑选若干支股票，作为量化基金的股票池。\n因为不同的行情，对应不同的模型，因此量化基金的调仓很频繁。所以我们挑选基金时，也没必要看成份股，反正没多久就换了。<img src=\"https://pic4.zhimg.com/50/v2-25cc6fad51a097b3e2ce5e01ef14e6f6_hd.jpg\" data-rawwidth=\"640\" data-rawheight=\"368\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-25cc6fad51a097b3e2ce5e01ef14e6f6_r.jpg\">选股风格九宫格\n如上图，这是量化基金的选股风格九宫格。不同的行情，风格也会在九个格子中切换。2.我们投资的收益分为两种：<img src=\"https://pic1.zhimg.com/50/v2-846881784adeba68ab85eded400ca0d8_hd.jpg\" data-rawwidth=\"640\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-846881784adeba68ab85eded400ca0d8_r.jpg\">贝塔收益，也就是市场的整体收益，代表标的是指数型基金。关键看的是，跟踪的精准程度；\n阿尔法收益，指跑赢市场的超额收益。假设今年市场的贝塔收益是10%，老钱做到了15%。那么，我的阿尔法收益就是5%。代表标的是股票型基金，当然，量化基金也是股票型基金的一种。\n评价阿尔法收益，关键看的是，跑赢了贝塔多少。打败市场，这是量化基金的根本目标。\n如上文所说，量化基金的模型经常变化，持股很分散，基金的整体业绩并不依赖单支股票的表现。\n所以量化基金每年的总收益经常会比不上当年重仓单一行业的热门基金。虽然不是明星，但它却是常胜将军。\n优秀的量化基金，确实可以常年跑赢市场，获得不错的阿尔法收益。3.接下来说说具体标的：<img src=\"https://pic4.zhimg.com/50/v2-1c0861e6c6ecd05e955f1ed64570c8ab_hd.jpg\" data-rawwidth=\"640\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-1c0861e6c6ecd05e955f1ed64570c8ab_r.jpg\">规模数据取自2016年12月末\n强调一点，去年的明星量化基金—长信量化先锋，我并没有推荐。原因基金规模太大，已经过百亿了。\n任何一个模型的容量都是有限的，理论上规模越小，管理难度也就越小。所以我挑选了三支规模相对较小的量化基金。\n来看下他们仨的表现：<img src=\"https://pic4.zhimg.com/50/v2-ee2aa26c48dc35f405b21565c005b377_hd.jpg\" data-rawwidth=\"640\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-ee2aa26c48dc35f405b21565c005b377_r.jpg\">\n我挑选了申万量化小盘163110跟中证500指数做了比较。如前面所说，打败市场是量化基金的根本目标。<img src=\"https://pic3.zhimg.com/50/v2-096af0ad8b2fa6049cd7e84576f5f602_hd.jpg\" data-rawwidth=\"640\" data-rawheight=\"197\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-096af0ad8b2fa6049cd7e84576f5f602_r.jpg\">蓝色线：申万量化小盘163110  \n灰色线：中证500指数  \n如上图，这支量化基金还是长期跑赢市场的。\n至于怎么买，依然是定投就好。此外，建议购买之前，扫一遍基金净值走势，自己在心里设置一个低点。\n比如申万量化小盘163110，我觉得净值跌到2块以下，可以大量加仓买入。多少是低点，见仁见智，没有标准答案。 更多内容，关注公众号「老钱说钱」", "answer_votes": "0", "answer_comment": "​添加评论"}
