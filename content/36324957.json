{"question_id": "36324957", "question_text": "数据分析师，工作中经常使用机器学习模型，但是以调库为主。自己一直也在研究算法，也裸写过一些经典的算法。最近在看PRML这类书籍，感觉有点吃劲，主要是公式推导看不懂，很多数学符号不知其意。也特地学过线性代数、微积分等，但是然并卵，还是看不懂大段的公式以及那些神奇的矩阵计算~机器学习新手，想前来问下，有没有哪些数学知识是可以弥补这一类缺陷的?能否具体的说一些知识点或者相关的文章和书籍？感谢~~", "question_title": "机器学习应该准备哪些数学预备知识？", "question_commet": "​3 条评论", "answer_number": "51 个回答", "question_followers": "5,931", "brower_number": "249,641"}
{"answer_author": "Jacky Yang", "answer_id": 255970074, "answer_text": "最近在给几位程序员朋友培训机器（深度）学习，而且10月份刚把数学基础部分培训完，看到这个问题，结合培训的感受，趁热写一点小小的心得体会。看了一下题主的问题，的确，现在很多想从事于机器学习的朋友都存在类似的困惑，主要是很多相关的书看不懂，尤其是数学部分，包括题主提到的PRML，还有最近的深度学习圣经。不得不说，这些书籍其实都很经典，但经典的书未必都适合每个人，毕竟这些著作其实是有一些门槛的，所以如何把这个门槛降低，或者换一个说法，如何把其中的数学基础用通俗易懂的语言解读出来，也是很有意义的一件事。我在培训当中也是深有体会。以下我假定读者跟题主情况类似：希望从事于机器学习，但数学多年不用，在阅读算法书籍的过程中，数学部分理解起来有难度。同时也欢迎业内朋友提供宝贵建议和意见。对于绝大多数从事于机器学习的人来说，学数学的目的，主要是便于（深入）理解算法的思路。那么问题来了，我们到底要把数学学到什么程度？我这里举几个例子：1.线性最小二乘法大家可以随意搜索一下，相关的文章很多。长篇大论的不少，刚入门的朋友一看到那些公式可能就看不下去了。比如下面的解释：<img src=\"https://pic4.zhimg.com/50/v2-c27d5b477063946b6abfd15b5a635115_hd.jpg\" data-caption=\"\" data-rawwidth=\"566\" data-rawheight=\"432\" class=\"origin_image zh-lightbox-thumb\" width=\"566\" data-original=\"https://pic4.zhimg.com/v2-c27d5b477063946b6abfd15b5a635115_r.jpg\">毫无疑问，这样的解释是专业的，严谨的。事实上，这是深度学习圣经里的解释。我并没有诋毁大师的意思，只是觉得用一个具体的例子来说明，可能会让读者更加容易理解：小明是跑运输的，跑1公里需要6块，跑2公里需要5块（那段时间刚好油价跌了），跑3公里需要7块，跑4公里需要10块，请问跑5公里需要多少块？如果我们有初中数学基础，应该会自然而然地想到用线性方程组来做，对吧。<img src=\"https://pic4.zhimg.com/50/v2-d0ddc1b3a34db919c948eb902ed03128_hd.jpg\" data-caption=\"\" data-rawwidth=\"132\" data-rawheight=\"25\" class=\"content_image\" width=\"132\">这里假定x是公里数，y是运输成本（β1和β2是要求的系数）。我们把上面的一组数据代入得到这么几个方程：<img src=\"https://pic3.zhimg.com/50/v2-31d99f9bb1f5b01407210aa4bbb33b0c_hd.jpg\" data-caption=\"\" data-rawwidth=\"150\" data-rawheight=\"132\" class=\"content_image\" width=\"150\">如果存在这样的β1和β2，让所有的数据（x，y）=（1,6），（2,5），（3,7），（4，10）都能满足的话，那么解答就很简单了，β1+5β2就是5公里的成本，对吧。但遗憾的是，这样的β1和β2是不存在的，上面的方程组很容易，你可以把前面两个解出来得到一组β1和β2，后面两个也解出来同样得到一组β1和β2。这两组β1和β2是不一样的。形象地说，就是你找不到一条直线，穿过所有的点，因为他们不在一条直线上。如下图：<img src=\"https://pic1.zhimg.com/50/v2-85dea13158982381d5b2712c6bf72261_hd.jpg\" data-caption=\"\" data-rawwidth=\"220\" data-rawheight=\"216\" class=\"content_image\" width=\"220\">可是现实生活中，我们就希望能找到一条直线，虽然不能满足所有条件，但能近似地表示这个趋势，或者说，能近似地知道5公里的运输成本，这也是有意义的。现实生活当中，有很多这样的例子，想起以前在某公司上班的时候，CEO说我们研发部做事有个问题：一个研发任务，要求三个月做完，因为周期太短，完成不了，就干脆不做，这显然是不对的，要尽全力，哪怕三个月完成了80%，或者最终4个月完成，总比不作为的好。其实最小二乘法也是这样，要尽全力让这条直线最接近这些点，那么问题来了，怎么才叫做最接近呢？直觉告诉我们，这条直线在所有数据点中间穿过，让这些点到这条直线的误差之和越小越好。这里我们用方差来算更客观。也就是说，把每个点到直线的误差平方加起来：<img src=\"https://pic3.zhimg.com/50/v2-939c9118667ee381725547fa605041dc_hd.jpg\" data-caption=\"\" data-rawwidth=\"540\" data-rawheight=\"68\" class=\"origin_image zh-lightbox-thumb\" width=\"540\" data-original=\"https://pic3.zhimg.com/v2-939c9118667ee381725547fa605041dc_r.jpg\">（如果上面的四个方程都能满足，那么S的值显然为0，这是最完美的，但如果做不到完美，我们就让这个S越小越好）接下来的问题就是，如何让这个S变得最小。这里有一个概念，就是求偏导数。这里我想提一下，在培训的过程中，我发现机器学习的数学基础课程当中，微积分是大家印象最深刻的，而且也最容易理解：比如导数就是求变化率，而偏导数则是当变量超过一个的时候，对其中一个变量求变化率。如果这个概念也忘了，可以参考我在深度学习回答里那个王小二卖猪的例子。这里就不细讲了：Jacky Yang：深度学习如何入门？要让S取得最小值（或最大值，但显然这个函数没有最大值，自己琢磨一下），那么S对于β1和β2分别求偏导结果为0，用一个直观的图来表示：<img src=\"https://pic3.zhimg.com/50/v2-9680853169bca0dccc69678c83387461_hd.jpg\" data-caption=\"\" data-rawwidth=\"300\" data-rawheight=\"225\" class=\"content_image\" width=\"300\">我们看到这条曲线，前半部分是呈下降的趋势，也就是变化率（导数）为负的，后半部分呈上升的趋势，也就是变化率（导数）为正，那么分界点的导数为0，也就是取得最小值的地方。这是一个变量的情况，对于多个变量的情况，要让S取得最小值，那最好是对β1和β2分别求导（对β1求导的时候，把β2当常量所以叫求偏导），值为0：<img src=\"https://pic4.zhimg.com/50/v2-9d9fd080c5bfb901174639a5f237cc69_hd.jpg\" data-caption=\"\" data-rawwidth=\"283\" data-rawheight=\"60\" class=\"content_image\" width=\"283\"><img src=\"https://pic4.zhimg.com/50/v2-4e459dd4457d9d35a97d555e20bbbe78_hd.jpg\" data-caption=\"\" data-rawwidth=\"313\" data-rawheight=\"60\" class=\"content_image\" width=\"313\">看到这个我们就熟悉了，两个变量，刚好有两个方程式，初中学过，那么很容易得出：<img src=\"https://pic1.zhimg.com/50/v2-f030a94cc8b70b83b98051084192139d_hd.jpg\" data-caption=\"\" data-rawwidth=\"85\" data-rawheight=\"25\" class=\"content_image\" width=\"85\"><img src=\"https://pic2.zhimg.com/50/v2-1bc7e49962ca21254b7ae97c793e5f0c_hd.jpg\" data-caption=\"\" data-rawwidth=\"85\" data-rawheight=\"25\" class=\"content_image\" width=\"85\">其实也就意味着<img src=\"https://pic4.zhimg.com/50/v2-4e2a75f1bd2c476f91e6f303031b772b_hd.jpg\" data-caption=\"\" data-rawwidth=\"145\" data-rawheight=\"25\" class=\"content_image\" width=\"145\">这个函数也就是我们要的直线，这条直线虽然不能把那些点串起来，但它能最大程度上接近这些点。也就是说5公里的时候，成本为3.5+1.4x5=10.5块，虽然不完美，但是很接近实际情况。在培训的过程中，一直在思考一个问题，也就是上面提到的那个，机器学习到底要把数学掌握到什么程度？首先我们得搞清楚我们到底要拿机器学习干什么，机器学习本来就是要通过分析现实生活中的数据得出其中的规律，以便为将来各方面提供指导意义。既然是这样，为何不直接从现实中来到现实中去，直接用数据和案例来讲解数学呢。我们显然不是为了学数学才学的机器学习，那就没必要堆砌哪些晦涩的公式了。除非我们要做纯理论研究。当然，数学的一些理念，思想或者精髓是需要掌握的，其实很多时候，我们都是在做不到完美的情况下，求那个最接近完美的解，别忘了机器学习很多情况下其实是在做拟合，所以说最小二乘法对于机器学习非常重要，这也是我把它当做第一个例子的原因。其实深度学习里的反向传播不也是一样么？刚开始不完美，但我要想办法让它越来越接近完美，预测值与实际值差距越来越小。所谓训练，其实也就是不断追求完美的一个过程。2.拉格朗日乘子法听到拉格朗日乘子法这个名字的时候，很多人的第一反应是：这玩意儿是不是很高深啊，先入为主地有了畏难的情绪。但我把它讲完以后，大部分人表示并不难，而且现实生活中，我们经常潜移默化会用到拉格朗日乘子法。甚至可以说，不用拉格朗日乘子法的人生都是不完整的人生。我们来看一下定义：<img src=\"https://pic1.zhimg.com/50/v2-467122fcd5221c0b8ff6f8d50b9b73c4_hd.jpg\" data-caption=\"\" data-rawwidth=\"802\" data-rawheight=\"283\" class=\"origin_image zh-lightbox-thumb\" width=\"802\" data-original=\"https://pic1.zhimg.com/v2-467122fcd5221c0b8ff6f8d50b9b73c4_r.jpg\">虽然这个定义应该说是很简洁明了的，但对于大部分人来说，依然还是有点懵。不太清楚为什么要这么做。拉格朗日到底要搞什么飞机？我们还是举个例子：某工厂在生产过程中用到两类原材料，其中一种单价为2万/公斤，另一种为3万/公斤，而工厂每个月预算刚好是6万。就像下面的公式：<img src=\"https://pic2.zhimg.com/50/v2-25d5bab53a960f193a1d7ebf194375a6_hd.jpg\" data-caption=\"\" data-rawwidth=\"230\" data-rawheight=\"30\" class=\"content_image\" width=\"230\">经过分析，工厂的产量f跟两种原材料（x1，x2）具有如下关系（我们暂且不管它是如何来的，而且假定产品可以按任意比例生产）：<img src=\"https://pic1.zhimg.com/50/v2-abb6ab9b9093e2a7ff000554c32b639d_hd.jpg\" data-caption=\"\" data-rawwidth=\"200\" data-rawheight=\"27\" class=\"content_image\" width=\"200\">请问该工厂每个月最少能生产多少？其实现实生活中我们会经常遇到类似的问题：在某个或某几个限制条件存在的情况下，求另一个函数的极值（极大或极小值）。就好比你要在北京买房，肯定不是想买什么房子就买什么房子，想买多大就买多大，而是跟你手头的金额，是否有北京户口，纳税有没有满五年，家庭开支/负担重不重，工作单位稳不稳定都有关系。回到工厂的例子，其实就是求函数f的极值。上面我们提到，极值点可以通过求偏导（变化率为0的地方为极值点）来实现，函数f（x1，x2）对x1，x2分别求偏导，那么得出的结论是：x1，x2都为0的时候最小，单独看这个函数，这个结论对的，很显然这个函数的最小值是0（任何数的平方都是大于或等于0），而且只有x1和x2同时为0的时候，取得最小值。但问题是它不满足上面的限制条件。怎么办呢？拉格朗日想到了一个很妙的办法，既然h（x1，x2）为0，那函数f（x1，x2）是否可以加上这个h（x1，x2）再乘以一个系数呢？任何数乘以0当然是0，f（x1，x2）加上0当然保持不变。所以其实就可以等同于求下面这个函数的极值：<img src=\"https://pic1.zhimg.com/50/v2-8e0b2cd7bfc1e3b28fb211c7a9006a8a_hd.jpg\" data-caption=\"\" data-rawwidth=\"343\" data-rawheight=\"32\" class=\"content_image\" width=\"343\">我们对x1，x2以及λ分别求偏导（极值点就是偏导数均为0的点）：<img src=\"https://pic2.zhimg.com/50/v2-b97298f93915b5ddc9a8f8cea9657692_hd.jpg\" data-caption=\"\" data-rawwidth=\"241\" data-rawheight=\"175\" class=\"content_image\" width=\"241\">解上面的方程组得到x1=1.071，x2=1.286 然后代入f（x1，x2）即可。这里为什么要多加一个乘子λ呢，试想一下，如果λ是个固定的数（比如-1），我们也能通过上面的方程式1,2求解得到x1，x2，但是我们就得不到方程式3，其实也就是没有约束条件了。所以看到没有，拉格朗日很聪明，他希望我们在求偏导（极值点）以后，还能保留原有的约束条件。我们上面提到，单独对函数求极值不能保证满足约束条件，拉格朗日这么一搞，就能把约束条件带进来，跟求其他变量的偏导结果放在一起，既能满足约束条件，又能保证是约束条件下的极值。借用金星的一句话：完美！当然这是一个约束条件的情况，如果有多个约束条件呢？那就要用多个不同的λ（想想为什么），正如最上面的那个定义那样，把这些加起来（这些0加起来也是0）。机器学习里的数学，我感觉只需要掌握里面这个核心思想即可，就像拉格朗日乘子法，求条件极值---》转化为求（函数+条件）的极值，每一步都很妙。其实我想说的是，体会这种妙处以后，再看SVM的算法，会感觉舒服很多，数学主要是为了让人更好地理解算法，并不是为了数学而学数学。人生苦短，还成天被晦涩的书籍所困扰，“感觉身体好像被掏空”，这样真的好么？3.朴素贝叶斯之所以把这个拎出来，是因为我一直想吐槽一下那个公式：<img src=\"https://pic1.zhimg.com/50/v2-f22ba452116a2b89416b612e36785f26_hd.jpg\" data-caption=\"\" data-rawwidth=\"640\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-f22ba452116a2b89416b612e36785f26_r.jpg\">我想吐槽，是因为几乎没有一篇文章解释这个公式是怎么来的。很多文章一上来就是这个公式。对于已经对条件概率没多少概念的朋友来说，脑子里其实一直有疑问。其实要解释并不难，把P(B)放到左边，除法改成乘法就容易理解多了。P(A|B) x P（B） = P(B|A) x P（A）也就是：B发生的概率 x B已经发生的情况下A发生的概率 = A发生的概率 x A已经发生的情况下B发生的概率。如果这个不好理解，我们还是举个例子：<img src=\"https://pic3.zhimg.com/50/v2-62cd08625329ab54cf578310d1d458de_hd.jpg\" data-caption=\"\" data-rawwidth=\"281\" data-rawheight=\"193\" class=\"content_image\" width=\"281\">如上图所示，口袋里有5个球（2个蓝色，3个红色），每次取一个，可能的结果如下图所示：<img src=\"https://pic2.zhimg.com/50/v2-4aac9308637faf9b6db09915ec15b684_hd.jpg\" data-caption=\"\" data-rawwidth=\"392\" data-rawheight=\"131\" class=\"content_image\" width=\"392\">第一次取出来是蓝色球的概率是2/5，我们把这个概率叫P(A)，然后在A发生的情况下，再取出一个红球的概率是多少？显然是3/4，因为只剩下3个红球一个蓝球，这个3/4就是P(B|A)，也就是在A发生的情况下，B发生的概率，这叫条件概率。我们把他们相乘得到：（2/5） x （3/4）=3/10接着我们换另一个方式算：如果第一次取到红球，第二次取到蓝球。同理，P(B)为3/5，P(A|B)为2/4，两个相乘：（3/5） x （2/4）=3/10他们是相等的。也就是说：P(A|B) x P（B） = P(B|A) x P（A）这个并不是特例，看下面的公式：<img src=\"https://pic2.zhimg.com/50/v2-2f420755a27ff9a90ef3e6b18e519798_hd.jpg\" data-caption=\"\" data-rawwidth=\"406\" data-rawheight=\"134\" class=\"content_image\" width=\"406\">事实上，P(A and B) 和P(B and A)是一样的，只是一前一后发生的顺序不同。我们把这个公式P(A|B) x P（B） = P(B|A) x P（A）的P(B)拿到另一边，这就是朴素贝叶斯的公式。除了上面几个例子，其实还有很多方面，都可以用不那么晦涩的方式去解读。比如高斯分布，很多文章，包括一些经典书籍，一上来就是那个公式：<img src=\"https://pic4.zhimg.com/50/v2-10ec6b000341687f59fa14feda68d707_hd.jpg\" data-caption=\"\" data-rawwidth=\"312\" data-rawheight=\"54\" class=\"content_image\" width=\"312\">然而很多人并不太明白，为何要用这样的分布，为什么叫正态分布，而不叫变态分布。其实它是大自然的一种普遍规律。其实可以用这个图：<img src=\"https://pic1.zhimg.com/50/v2-4b6eb719957ecf398266e1494c83f08e_hd.jpg\" data-caption=\"\" data-rawwidth=\"470\" data-rawheight=\"216\" class=\"origin_image zh-lightbox-thumb\" width=\"470\" data-original=\"https://pic1.zhimg.com/v2-4b6eb719957ecf398266e1494c83f08e_r.jpg\">这是统计学生两门学习成绩总和（总分200分），横轴是分数，纵轴是所占的比例。我们发现，学霸和学渣都比较少，大部分人都集中在150分左右（很显然，大部分人都是你我这种普通人嘛），如果统计样本足够大，以至于达到无穷，那就变成了钟形曲线。普通人的平均分数，就是高斯分布里的那个μ，也就是均值，而那个σ怎么解释呢？就是你这个曲线越陡，σ越小，这个叫方差。试想一下，如果大家都挤成一坨，成绩都差不多，差别小，也就是方差小，中间的方块占的比例就越高，当然就越陡了。如下图：<img src=\"https://pic3.zhimg.com/50/v2-5deec6c0e8121974072b8f881d7d0bb7_hd.jpg\" data-caption=\"\" data-rawwidth=\"792\" data-rawheight=\"415\" class=\"origin_image zh-lightbox-thumb\" width=\"792\" data-original=\"https://pic3.zhimg.com/v2-5deec6c0e8121974072b8f881d7d0bb7_r.jpg\">另外，还有一些概念，比如正交，很多朋友问起过这个问题：Jacky，向量正交的概念我在大学里学过，但就是不知道为啥要正交？其实我们要理解正交，可以先理解什么是相交，两条直线相交表明存在一定的夹角，但这个夹角可大可小，如果是0的情况下，他们是在一条线上的（向量都是过原点的，这里我们不考虑不过原点的情况），180度的时候也是在一条直线上，这个两种情况我们都可以认为他们是线性相关的，那么什么时候，最不相关呢，很显然是90度的时候，也就是垂直的时候。除了垂直和平行的情况，夹角在0-90度或者90度到180度之间的情况，相关性介于垂直和平行之间。我们试想一下，如果我们要把一组数据分解成不同的特征，我们希望每个分量各自具有独立的特点呢？还是希望每个分量，你中有我，我中有你好呢？显然是越无关越好，如果他们之间太“暧昧”，就没有特点了。最好是各个分量，两两互相垂直。当然，垂直是几何上的解释，对于向量来说，更严谨的说法（多维）就是正交。关于机器学习中数学的通俗化表达，限于篇幅（太长看了也累），先聊到这里，目前还在继续整理当中，想到哪说到哪，思路还不够清晰，希望在本次培训结束以后，能整理出一个完整的版本。同时也请业内朋友多提宝贵意见和建议。如果在阅读PRML和deep learning的过程中，对有些数学部分不太清楚，也请在评论区留言或者私信给我也可以。请列出具体的内容或对应的书的页数。最近在写一本小册子，也很希望收到朋友们的需求，痛点及反馈。谢谢。关于数学基础课程列表，几个高票答案总结的很全了，这里我就不重复贴了。不过有人总结了一份文档，里面列出了机器学习中用到的数学基础，虽然没有详细描述，但思路清晰，简洁明了，可以参考：http://www.cogsci.ucsd.edu/~ajyu/Teaching/Cogs118A_wi10/Refs/basic_math.pdf", "answer_votes": "540", "answer_comment": "​51 条评论"}
{"answer_author": "Jiawei Fan", "answer_id": 66989594, "answer_text": "--2015-10-13 再更新--哦对了，正文里面提到了三种矩阵变换，下面给大家玩一个趣味题目(✿◡‿◡)以下两个图分别对应了“三种矩阵变换”里的哪种矩阵变换，然后，你可以猜猜这两个变换矩阵具体值是多少，提示：很特殊的矩阵哦~~~~<img src=\"https://pic2.zhimg.com/50/c839a115c88ae78c5f27b547249bb02c_hd.jpg\" data-rawwidth=\"2400\" data-rawheight=\"1213\" class=\"origin_image zh-lightbox-thumb\" width=\"2400\" data-original=\"https://pic2.zhimg.com/c839a115c88ae78c5f27b547249bb02c_r.jpg\"><img src=\"https://pic2.zhimg.com/50/db50f00620903d5b2496bca272f2c866_hd.jpg\" data-rawwidth=\"2400\" data-rawheight=\"1213\" class=\"origin_image zh-lightbox-thumb\" width=\"2400\" data-original=\"https://pic2.zhimg.com/db50f00620903d5b2496bca272f2c866_r.jpg\">--2015-10-13 更新--感谢那么多童鞋的点赞，第一次在知乎上拿到100+的赞，O(∩_∩)O~~有加我微信的童鞋有好多问我要机器学习资料的，我就在这里统一回复一下：书籍的话大家自行百度“书名+pdf+微盘”应该都有，当年我就是这么下的机器学习：斯坦福大学公开课 ：机器学习课程Pattern Recognition and Machine Learning线性代数：麻省理工公开课：线性代数Introduction to Linear Algebra, 4th edition ,GILBERT STRANG凸优化Convex Optimization,Stephen Boyd概率这边我就不单独推荐书籍了，一方面自己没有遇到非常惊艳的相关书籍（大家可以推荐给我），另一方面无论ng的公开课，还是prml，概率部分还是蛮详细的，个人经验是概率部分不是非常需要单独学习。----------------------------题主好，关注你已经有些时间了，是一个执着于数据领域的人。恰好我本人的业余时间几乎都用来学习prml了，目前大概看了8个章节，每个章节大概精读（较好地理解）三分之二的内容，所以就回答一下这个问题。首先，题主说的没错，线性代数和微积分都是必要的，但是初学者容易割裂地看待它们以及机器学习，不清楚哪些线性代数&微积分的知识才是掌握机器学习数学推导的关键。一样，我也走过并继续在走很多弯路，就说说我的感受吧，大家一起探讨探讨。线性代数内容：推荐Introduction to Linear Algebra,4th edition网易公开课地址：麻省理工公开课：线性代数作者&讲课者：麻省理工的一个非常有趣幽默，人格魅力爆棚&解析集聚洞察力直指根本，把国内大部分线性代数教材轰的渣也不剩的老教授Gilbert Strang。1 理解矩阵变换矩阵变换简单的说就是x->Ax，A矩阵把原空间上的向量x映射到了Ax的位置，看似简单实在是奥妙无穷。1.1 A可以是由一组单位正交基组成，那么该矩阵变换就是基变换，简单理解就是旋转坐标轴的变换，PCA就是找了一组特殊位置的单位正交基，本质上就是基变换。1.2 A可以是某些矩阵，它们在某些特殊的方向上只对x做了收缩拉伸变换而没有改变方向，简单来说就是，这些特殊的方向x就是特征向量，而就是收缩拉伸的量，描述了这些特殊的方向上的变换后，其实我们很容易画出这种矩阵变换的几何图解。1.3 A可以是投影矩阵，把x投影到某个直线上，或者某个subspace上，线性回归模型有最小二乘解释，最小二乘可以由极大似然函数推得，当然还能用投影矩阵解释。2 理解（对称）矩阵的特征向量特征值分解2.1 对称矩阵特征分解是理解多维高斯分布的基础要理解多维高斯分布需要四个知识：等值面，对称矩阵特征分解，正交基变换，多维椭圆方程2.2 对称矩阵特征分解对称矩阵特征分解可以直截了当的导出矩阵对角化的公式，而对协方差矩阵的对角化又是PCA的核心数学知识理解PCA的数学基础：协方差矩阵对角化，基变换矩阵。3 一些线性代数的嗅觉素养其实很多感觉是逐步形成的，比如n维向量x乘以x的转置就是一个对称矩阵等…4 本质& 洞悉本质下面抛开机器学习，回归到线性代数本身，我现在回顾，还是可以清晰的感觉到，理解&掌握线性代数的几个不同的阶段（或者说坎在哪里），我把它们总结成几个小问题，大家也可以自测一下，如果你扪心自问能够很好的回答其中的某个问题，那么相当于你在线性代数的某一块知识领域里已经相对纯熟&洞悉到非常基础但是最核心的本质思想。这种东西大学教材真的给不了，也不是你做几张线性代数试卷，考个100分能够比拟的，本质的东西需要思考，体会，顿悟，了然一笑，一切尽在不言中…话也说回来我痴迷机器学习原理，痴迷数学，说到底还是想要多体验这种感觉，会上瘾的…问题一，你有感觉到某一类矩阵和矩阵相乘，其实就是解方程时的消元吗？问题二，你有发现解方程时对矩阵的操作，与消元法解方程的对应关系吗？你有发现行列式的定义和性质，与消元法解方程的对应关系吗？你有发现求逆矩阵与消元法解方程的对应关系吗？而奇异矩阵与这个消元法解方程又有什么关系呢？你有发现非常自然的消元法解方程，是连结矩阵、行列式、逆矩阵这些概念线索和纽带吗？这么普普通通的消元法解方程是多少线性代数基础概念的核心啊！所有的东西都不是无中生有的，线性代数的设定真的不是像国内那些垃圾教材里面描述的好像一只孙猴子一样，像直接从石头缝里蹦出来的啊！问题三，前面已经提到了，三种“理解矩阵变换”，你理解了吗？问题四，为什么行秩和列秩是一样的？涉及四个基本子空间（列空间，零空间，行空间，左零空间），这个东西是我最近才感悟到的。线性代数部分先总结到这里，后面还有概率统计和微积分部分，就简略说一下，以后有时间再补充。概率统计：（1）      极大似然思想（2）      贝叶斯模型（3）      隐变量混合概率模型，EM思想基础的典型分布是逃不过的，尤其高斯分布。微积分：主要体现在极值问题 与 （条件）最优化问题偏导数，梯度这两个概念必须深入人心还有就是凸优化和条件最优化问题，这个是理解SVM，或者线性回归等等模型正则化的基础。。。我的微信号mubing_s，我平日里如果某一块知识点（面）想清楚了，一遍会用白纸黑字写下来记录备忘，有机器学习 & 数学方面的，有兴趣想看的、想探讨同学，都欢迎加我哦，:-D先总结到这里了，欢迎大家拍砖！以后有时间详细补充。", "answer_votes": "913", "answer_comment": "​46 条评论"}
{"answer_author": "留德华叫兽", "answer_id": 254938339, "answer_text": "利益相关：楼主 @Robin Shen 以本科应用数学和硕士运筹学、优化理论的背景转到德国海德堡大学读博，主要从事机器学习、计算机视觉的研究，希望自己的一些经验可以对想入门机器学习的朋友们有点借鉴作用。此回答的部分答案摘自我另外一个相关回答：Robin Shen：想转专业机器学习（人工智能）需要学哪些课程？首先对人工智能、机器学习一个综述：大话“人工智能、数据科学、机器学习”--综述 - 知乎专栏笼统地说，原理和基础都在数学这边，当然有很多偏应用和软件使用的技术，例如“深度学习调参”等，这些报个培训速成班就能学会的技术含量不那么高的东西，不在讨论范围内。这里要讨论的，是如何系统的学习，然后自己能编出这机器学习或深度学习的程序或软件－－我想，这才能称为一个合格的机器学习、数据科学家。入门基础1, 微积分（求导，极限，极值）和线性代数（矩阵表示、矩阵运算、特征根、特征向量）是基础中的基础，某篇图像分割1w+引用的神文核心思想便就求解构造矩阵的特征向量；2,  数据处理当然需要编程了，因此C/C++/Python任选一门（推荐Python，因为目前很多库和Library都是用python封装），数据结构可以学学，让你编程更顺手更高效，但是编程不是数据处理的核心。当然了，楼主所在的图像处理界，熟练使用matlab或者Python调用opencv库是必要条件，但是again他们只是工具，业余时间自学，多练练就没问题。有同学问用R行不行，补充一点，用什么编程语言很大部分取决于你的核心算法会调用什么已有的库函数，比如楼主的科研里面核心算法往往是MIP（混合整数规划）问题需要调用Cplex或Gurobi库函数，因此C/C++/Python/Java这些和Cplex接口良好的语言都可以拿来用，这时候R就别想了。(更新：最新Gurobi版本支持R)另外虽然图像处理界一些open-source的code都用C++写的，但是鉴于使用方便都会提供Python的接口，因此需要用到这些code的话，用Python调用比较方便；但是，如果是高阶骨灰级玩家，需要修改甚至自己写源代码，那么还是推荐C/C++，因为他们的速度最快。3，算法通常高校都会有算法类的课程，会概述各类算法的基础和应用，其中包括：精确算法、近似算法、启发式算法、演化算法、递归算法、贪婪算法等待，还有各类优化算法。算法非常核心，想必大家都听说过算法工程师这个职位。关于数学模型和算法的区别、联系，参见：【学界】整数规划精确算法/近似算法/(元)启发算法/神经网络方反向传播等算法的区别与关联中级教程1，概率论+统计（很多数据分析建模基于统计模型）、统计推断、随机过程等2，线性规划+凸优化（或者只学一门叫numerical optimization，统计、机器学习到最后就是求解一个优化问题）、非线性规划等3，数值计算、数值线代等当年我是在数学系学的这门课，主要是偏微分方程的数值解。但我觉得其开篇讲的数值计算的一些numerical issue更为重要，会颠覆一个数学系出身小朋友的三观。(原来理论和现实差距可以这么大！)Conditional number, ill-conditioned problem，会让你以后的编程多留个心眼。恭喜你，到这里，你就可以无压力地学习Machine Learning这门课了（其实机器学习，通篇都是在讲用一些统计和优化来做clustering 和 classification这俩个人工智能最常见的应用）。并且你就会发现，ML课中间会穿插着很多其他课的内容。恩，知识总是相通的嘛，特别是这些跨专业的新兴学科，都是在以往学科的基础上由社会需求发展而来。到这里，其实你已经能看懂并且自己可以编写机器学习里面很多经典案例的算法了，比如regression，clustering，outlier detection。关于优化类课程的综述，欢迎关注我的专栏：[运筹帷幄]大数据和人工智能时代下的运筹学 - 知乎专栏运筹学（最优化理论）如何入门？ - 知乎学到Mid-level，就已经具备绝大部分理论基础了。然后做几个实际项目，就能上手然后就可以“吹嘘”自己是搞机器学习的，就能找到一份工作了。但是要读Phd搞机器学习的科研，那么高阶课程必不可少，而且同一个topic你需要上好几门课，并且你博士的课题，很可能只是一本书中一个章节里面一小节里讲的算法，去改进他。比如，楼主的博士课题就是mixed linear programming + discrete graphical models + markov random fields + regression + clustering + segmentation。高阶课程再高阶的课程，就是比较specific的课程了，可以看你做的项目或者以后的concentration再选择选修，比如：Probabilistic Graphical Models（概率图模型）， Integer Programming（整数规划） ，计算机视觉，模式识别，视频追踪，医学图像处理，增强学习，深度学习, 神经网络，自然语言处理，网络信息安全，等等等等。深度学习：目前非常火，打败了非常多几十年积累起来的经典方法。增强学习：也很火，游戏AI、自动驾驶、机器人等等，它都是核心。概率图模型：深度学习之前非常popular的“学习”方法，有严格的数学模型和优美的算法，虽然目前被前俩者盖过了风头，但是依然有它的立足之处。什么？你不知道最近用PGM发了篇Nature，打败了CNN？快看下面：Robin Shen：如何评价 Vicarious 在 Science 上提出基于概率图模型（PGM）的 RCN 模型？再比如有用偏微分方程做图像处理的（比较小众），那么这时候你肯定要去学一下偏微分方程了，大都是以科研为主导的。科研嘛，为了发文章，就是要尝试前人没尝试过的方法，万一效果不错呢，就是一篇好paper了，对吧。附上顶尖会议排名，共勉：国际“顶尖”计算机视觉、机器学习会议大搜罗--附排名&接收率互联网教学资源书目没有特别推荐的，但是建议看英文原版。另外直接翻墙Youtube看视频课程，很多国际知名教授都很无私地把自己上课的视频放在youtube上免费学习（搜索我上面列出的科目名字）。如果确实要楼主推荐，那就推荐海德堡大学历史上最年轻的教授 Fred的机器学习视频（我基本都看过）：https://www.youtube.com/playlist?list=PLuRaSnb3n4kSgSV35vTPDRBH81YgnF3Dd另外一个教授给你上课的时候，开头一般是会推荐书给你的（如果你确实喜欢看书的话）。当然了，翻墙是楼主suppose你们需要拥有的基本生存技能。（注：以下再推荐一些视频，仅受之以渔，多为graduate course）1，Machine Learning by Prof. Nando de Freitas， 此视频是其在UBC时13年所录，后来跳槽去牛津计算机系了。https://www.youtube.com/playlist?list=PLE6Wd9FR--EdyJ5lbFl8UuGjecvVw66F62，Deep learning at Oxford 2015   by Prof. Nando de Freitas， 跳槽到牛津所录。https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu3，Probabilistic Graphical Models by Daphne Koller, 斯坦福大学计算机系教授https://www.youtube.com/playlist?list=PL50E6E80E8525B59C更多人工智能、优化理论的知识，尽在：[运筹帷幄]大数据和人工智能时代下的运筹学关于入行后就业前景（包括第三条运筹学、算法工程师），参见：国内(全球)TOP互联网公司、学术界超高薪的揽才计划有哪些？ - 知乎关于机器学习在咨询行业的应用，参见Data Science/Analytics 出身，可以在咨询行业做些什么？ - Ruobing Shen 的回答最后是通往大洋彼岸高薪博士职位，以及人工智能数据科学家的传送门：欧洲、北美、全球留学及数据科学深度私人定制咨询，从此DIY - Ruobing Shen的文章 - 知乎专栏", "answer_votes": "161", "answer_comment": "​25 条评论"}
{"answer_author": "阿里云云栖社区", "answer_id": 262804424, "answer_text": "线性代数在深度学习领域有着举足轻重的作用，它是深度学习各种算法的基本数学工具。云栖君给题主推荐的这篇文就是对深度学习中会用到的一些线性代数的基本概念和运算操作进行了介绍，希望能让有兴趣的同学一起来进行学习。作者Brendan Fortuner 是一名在西雅图的亚马逊的软件工程师，目前自己在人工智能方面进行研究。<img src=\"https://pic2.zhimg.com/50/v2-4345a14ce9557e2c7723114164fd7bb7_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"163\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-4345a14ce9557e2c7723114164fd7bb7_r.jpg\">上过Jeremy Howard的深度学习课程后，我意识到我在线性代数方面的不足，而这大大影响我对类似反向传播这样的概念的理解。因此我决定在这个方面花点时间，以补全这方面的知识。 本文是对线性代数的基本介绍，用于深度学习中会使用到的一些常见的线性代数操作。什么是线性代数？ 在深度学习的背景下，线性代数是一个数学工具，它提供了有助于同时操作数组的技术。 它提供了像向量和矩阵（电子表格）这样的数据结构用来保存数字和规则，以便进行加，减，乘，除的运算。线性代数为什么有用？ 线性代数可以将复杂的问题简单化，让我们能够对问题进行高效的数学运算。 以下是线性代数如何达到这些目标的一个例子。# Multiply two arrays \nx = [1,2,3]\ny = [2,3,4]\nproduct = []\nfor i in range(len(x)):\n    product.append(x[i]*y[i])\n# Linear algebra version\nx = numpy.array([1,2,3])\ny = numpy.array([2,3,4])\nx * y\n初始化这两个数组后，用线性代数的方法会快3倍。 如何在深度学习中使用线性代数？神经网络将权重存储在矩阵中。 线性代数使矩阵运算变得更加快捷简便，尤其是在GPU上进行训练的时候。 实际上，GPU是以向量和矩阵运算为基础的。 比如，图像可以表示为像素数组。视频游戏使用庞大且不断发展的矩阵来产生令人炫目的游戏体验。 GPU并不是处理单个像素，而是并行地处理整个像素矩阵。向量向量是1维数组。 在几何中，向量将大小和方向的潜在变化存储到一个点。 例如，向量[3，-2]表示向右移3个单位距离和向下移2个单位距离。而具有多个维度的向量称为矩阵。向量表示我们可以以不同的方式来表示向量。 这里有几个常见的表示方式。<img src=\"https://pic4.zhimg.com/50/v2-37f81e1d6a144a4f22ff9861c3a59f56_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"103\" class=\"content_image\" width=\"356\">几何中的向量向量通常表示从一个点出发的运动。 它们将大小和方向的潜在变化存储到一个点。 向量[-2,5]表示左移2个单位，向上5个单位。 参考资料。<img src=\"https://pic4.zhimg.com/50/v2-f637da0e3fcb53df949b31f343d87f1c_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"225\" class=\"content_image\" width=\"360\">向量可以应用于任何空间点。 向量的方向就是向上5个单位和向左2个单位的斜线，它的大小等于斜线的长度。标量操作标量运算涉及向量和某个数字。 我们可以通过对向量中的所有项进行加，减，乘，除操作来对其进行修改。<img src=\"https://pic4.zhimg.com/50/v2-c68e0b91696ed5db38a0ec9e895cf203_hd.jpg\" data-size=\"normal\" data-rawwidth=\"212\" data-rawheight=\"128\" class=\"content_image\" width=\"212\">Scalar addition 元素操作 在诸如加法，减法和除法的元素操作中，相应位置的值被重新组合以产生新的向量。 向量A中的第一个值与向量B中的第一个值配对。第二个值与第二个值配对，依此类推。也就是说，这两个向量必须有着相同的尺寸，才能完成元素操作*。<img src=\"https://pic3.zhimg.com/50/v2-a74d5f9d6ffff8320a08fc056c28d646_hd.jpg\" data-size=\"normal\" data-rawwidth=\"392\" data-rawheight=\"128\" class=\"content_image\" width=\"392\">Vector addition y = np.array([1,2,3])\nx = np.array([2,3,4])\ny + x = [3, 5, 7]\ny - x = [-1, -1, -1]\ny / x = [.5, .67, .75]\n*请参阅下面关于numpy 中的broadcasting方法详细信息。向量乘法 向量乘法有两种类型：点积和Hadamard乘积。点积 两个向量的点积是一个标量。 向量和矩阵的点积（矩阵乘法）是深度学习中最重要的操作之一。<img src=\"https://pic2.zhimg.com/50/v2-17271cbb13fefb2efeb0a172ffdfffd6_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"87\" class=\"content_image\" width=\"372\">y = np.array([1,2,3])\nx = np.array([2,3,4])\nnp.dot(y,x) = 20\nHadamard乘积 Hadamard乘积是元乘法，它的输出是一个向量。 <img src=\"https://pic4.zhimg.com/50/v2-2fd791609a8650444caed4e0d2ed6463_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"337\" data-rawheight=\"87\" class=\"content_image\" width=\"337\">y = np.array([1,2,3])\nx = np.array([2,3,4])\ny * x = [2, 6, 12]\n向量场 如果我们对一个点（x，y）应用一个加法或乘法的向量函数，向量场则表示了该点可能会移动多远。 给定空间中某一个点，向量场显示了图中各个不同点可能的变化力度和方向。<img src=\"https://pic2.zhimg.com/50/v2-97e0c1c141e3719fbb25bb45d5bf50c8_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"250\" data-rawheight=\"250\" class=\"content_image\" width=\"250\">参考 向量场是非常有趣的，因为它根据不同的起点可以向不同的方向移动。 这是因为向量场背后的向量存储着2x或x²这样的函数关系，而不是像-2和5这样的标量值。对于图上的每个点，我们将x值代入2x或x²，并从起始点绘制箭头指向新的位置。向量场对于类似梯度下降(Gradient Descent)这类的机器学习技术的可视化是非常有用的。矩阵 矩阵是数字或字符的矩形网格（如Excel表格），并具有加，减，乘等运算规则。矩阵维度 我们用列和行来描述矩阵的维度。 <img src=\"https://pic3.zhimg.com/50/v2-54dd7b1e3058615d29e31ae4edf03640_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"191\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic3.zhimg.com/v2-54dd7b1e3058615d29e31ae4edf03640_r.jpg\">a = np.array([\n [1,2,3], \n [4,5,6]\n])\na.shape == (2,3)\nb = np.array([\n [1,2,3]\n])\nb.shape == (1,3)\n矩阵标量运算矩阵的标量运算与向量一样。 简单地将标量应用于矩阵中的每个元素进行加，减，乘，除等操作。<img src=\"https://pic1.zhimg.com/50/v2-5eae0356cecc3a0a899b318d05ede15d_hd.jpg\" data-size=\"normal\" data-rawwidth=\"316\" data-rawheight=\"128\" class=\"content_image\" width=\"316\">Matrix scalar addition 矩阵单元操作 为了对两个矩阵进行加，减或除法，它们必须具有相等的维度。*我们以元素组合的方式产生对应的值，得到新的矩阵。<img src=\"https://pic4.zhimg.com/50/v2-f8996f3f58602bee6d3d790d5b89c6f8_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"514\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb\" width=\"514\" data-original=\"https://pic4.zhimg.com/v2-f8996f3f58602bee6d3d790d5b89c6f8_r.jpg\">a = np.array([\n [1,2],\n [3,4]\n])\nb = np.array([\n [1,2],\n [3,4]\n])\na + b\n[[2, 4],\n [6, 8]]\na — b\n[[0, 0],\n [0, 0]]\nNumpy 的broadcasting方法*这是个不得不提的话题，因为它在实践中非常重要。 在numpy中，元素操作的维度要求通过称为broadcasting的机制来扩展。 如果每个矩阵（行与行，列与列）中的相应维度满足以下要求，则这两个矩阵是兼容的：1.Â Â Â Â  两个矩阵维度相等，或2.Â Â  一个矩阵的维度为1a = np.array([\n [1],\n [2]\n])\nb = np.array([\n [3,4],\n [5,6]\n])\nc = np.array([\n [1,2]\n])\n# Same no. of rows\n# Different no. of columns\n# but a has one column so this works\na * b\n[[ 3, 4],\n [10, 12]]\n# Same no. of columns\n# Different no. of rows\n# but c has one row so this works\nb * c\n[[ 3, 8],\n [5, 12]]\n# Different no. of columns\n# Different no. of rows\n# but both a and c meet the \n# size 1 requirement rule\na + c\n[[2, 3],\n [3, 4]]\n但在更高的维度上（3维或4维），事情会变得有点奇怪，但是现在我们不用担心。 了解二维上的操作是个很好的开始。矩阵Hadamard乘积 矩阵的Hadamard乘积是一个元素运算，就像向量一样。 相应位置的值通过乘法运算来产生一个新的矩阵。<img src=\"https://pic2.zhimg.com/50/v2-4de232ee21a8a856880ad2d19adbe0db_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic2.zhimg.com/v2-4de232ee21a8a856880ad2d19adbe0db_r.jpg\">a = np.array(\n[[2,3],\n [2,3]])\nb = np.array(\n[[3,4],\n [5,6]])\n# Uses python's multiply operator\na * b\n[[ 6, 12],\n [10, 18]]\n只要矩阵维度符合broadcasting要求，就可以用Numpy对矩阵和向量进行Hadamard乘积运算。<img src=\"https://pic4.zhimg.com/50/v2-83bef6bce736132932fc9a8b79459d72_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb\" width=\"524\" data-original=\"https://pic4.zhimg.com/v2-83bef6bce736132932fc9a8b79459d72_r.jpg\">矩阵转置 神经网络经常处理维度不符合要求的矩阵。 而矩阵转置提供了一种方法来“旋转”其中一个矩阵，以使其操作符合乘法要求。 转置矩阵有两个步骤：1. 矩阵旋转90°2.反转每行元素的顺序（例如[a b c]变为[c b a]）例如，将矩阵M转置为T： <img src=\"https://pic2.zhimg.com/50/v2-74ec23a20f2f224fce43180b86d2e86d_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"261\" data-rawheight=\"233\" class=\"content_image\" width=\"261\">a = np.array([\n   [1, 2], \n   [3, 4]])\na.T\n[[1, 3],\n [2, 4]]\n矩阵乘法 矩阵乘法规定了一组对矩阵进行乘法运算，以产生新矩阵的规则。规则 并不是所有的矩阵都能进行乘法运算的。 并且，对输出矩阵的维度也存在要求。参考资料1.Â Â Â Â  第一矩阵的列数必须等于第二个矩阵的行数2.Â Â  M×N矩阵和N×K矩阵的乘积是M×K矩阵。 新矩阵取第一个矩阵的行和第二个矩阵的列。步骤矩阵乘法依赖于点积与行列元素的各种组合。 以下图为例(取自Khan学院的线性代数课程)，矩阵 C中的每个元素都是矩阵A中行与矩阵B中列的点积。 <img src=\"https://pic2.zhimg.com/50/v2-fc396c4cbc5d8c23bcb757ba7d0927b1_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"370\" data-rawheight=\"178\" class=\"content_image\" width=\"370\">操作a1·b1表示我们取矩阵A中第一行（1,7）和矩阵B中第1列（3,5）的点积。<img src=\"https://pic3.zhimg.com/50/v2-b7af2f761468a566cfe18eb773605ffc_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"660\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb\" width=\"660\" data-original=\"https://pic3.zhimg.com/v2-b7af2f761468a566cfe18eb773605ffc_r.jpg\">这里是另一种方法：<img src=\"https://pic3.zhimg.com/50/v2-d79f8f02c6f06ec95d5ac135489d1449_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"587\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb\" width=\"587\" data-original=\"https://pic3.zhimg.com/v2-d79f8f02c6f06ec95d5ac135489d1449_r.jpg\">为什么矩阵乘法以这种方式工作？ 矩阵的乘法运算非常有用。但背后并没有太深奥的数学规律。 之所以数学家发明了这种运算，完全是因为它简化了以前乏味的计算。 这是一个人为的产物，但却非常有效。用一下几个例子自我测试一下<img src=\"https://pic1.zhimg.com/50/v2-9f96998a5c962b33b51c8c1eb2278b64_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"1033\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic1.zhimg.com/v2-9f96998a5c962b33b51c8c1eb2278b64_r.jpg\">矩阵乘法与Numpy Numpy使用函数np.dot（A，B）进行向量和矩阵乘法运算。 它有一些其他有趣的功能和问题，所以我希望大家能在使用前阅读一下相关文档。 更多英文教程 Khan Academy Linear AlgebraDeep Learning Book Math SectionAndrew Ng’s Course NotesExplanation of Linear AlgebraExplanation of MatricesIntro To Linear AlgebraImmersive Math文章原标题《Linear algebra cheat sheet for deep learning》，作者：Brendan Fortuner，译者：friday012，审阅：李烽文章为简译，更为详细的内容，请查看原文更多技术干货敬请关注云栖社区知乎机构号：阿里云云栖社区 - 知乎", "answer_votes": "131", "answer_comment": "​9 条评论"}
{"answer_author": "知乎用户", "answer_id": 67009310, "answer_text": "Andrew Ng的课算是有良心了。 简单版去coursera上看，每周会有optional的section，就是补基础知识。 同样内容广度深度更野一点搜CS229 Stanford。这是coursera那门课的真正形态。 网站里有个链接叫handouts。section notes里面都是周五TA section的东西。这门课每周五会有一个TA带着大家补基础知识的section。 基础知识和目标知识不要分开来看，看到问题了，再去补，不然干看基础知识没有实际应用的理解效果很不好。想想高中数学和后来的高数，哪些不是好东西，但不配合应用去学没什么效果。 这门课相当成熟，每堂课都有完全拿你当SB一样详细的note pdf。强烈推荐。不建议系统的看数学书，如果你上过数学课的话。看书太费时间了，而且还是那句话，不是learn by doing，看十成记五成，理解也就三成吧。", "answer_votes": "291", "answer_comment": "​19 条评论"}
{"answer_author": "机器之心", "answer_id": 232838379, "answer_text": "机器之心整理本文作者依据自身经验给出了一套快速上手的可行方法及学习资源的分类汇总，机器之心在其基础上做了增益，希望对读者有所帮助。先决条件机器学习的基础是数学。数学并非是一个可选可不选的理论方法，而是不可或缺的支柱。如果你是一名计算机工程师，每天使用 UML、ORM、设计模式及其他软件工程工具／技术，那么请闭眼一秒钟，忘掉一切。这并不是说这些概念不重要，绝不是！但是机器学习需要一种不同的方法。如今 Python 如此流行的原因之一是其「原型设计速度」。在机器学习中，一种使用几行代码即可建模算法的语言绝对是必要的。微积分、线性代数、概率论在机器学习几乎所有算法中不可或缺。如果你的数学背景很扎实，请跳过这一章节。如若不然，那么重新温习一下这些重要概念也不错。考虑到理论的数量，我并不建议大家从大部头开始。尽管一开始可以用它查询具体概念，但是初学者先关注简单的话题比较好。网上有很多好的在线资源（比如 Coursera、可汗学院或优达学城），实用且适合各种背景的人群。但是我建议从提纲之类的简明书籍上手，其中所有核心概念均被涉及，次要概念可在需要的时候自行查询。这种方法虽然不够系统，但却避免了这样的缺陷：大量晦涩概念使得没有扎实理论背景的人望而却步。初学者最好先学习下列内容：概率论离散型和连续型随机变量主要分布（伯努利分布、二项式分布、正态分布、 指数分布、 泊松分布、Beta 和 Gamma 分布）矩估计和最大似然估计贝叶斯统计相关性系数和协方差（Correlation and Covariance）线性代数向量和矩阵矩阵的行列式特征向量和特征值矩阵分解（如 SVD）微积分极限与导数微分和积分数值计算与最优化方法网上有很多免费资源，比如《概率论入门》，Grinstead、Snell 著（https://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf）《线性代数入门》，Wise、Gallagher 著（http://www.stat.columbia.edu/~liam/teaching/4315-spr06/LinAlg.pdf）《微积分入门》，Heinbockel 著（http://www.math.odu.edu/~jhh/Volume-1.PDF）维基百科上也有很多好资源，对方程、定理等进行了清晰易懂的解释。机器之心也介绍过许多数学基础与概念：基础入门：深度学习矩阵运算的概念和代码实现想了解概率图模型？你要先理解图论的基本定义与形式深度神经网络中的数学，对你来说会不会太难？Reddit 热门话题：如何阅读并理解论文中的数学内容？机器学习主要需要的数学基础就是微积分、线性代数、概率论，我们感觉只需要掌握大学中常见的高数、线性代数、概率论与数理统计三门课程，基本上概念的理解就没什么问题了。如果再学一点数值计算和最优化等，我们基本上就能理解机器学习的学习过程推导。机器学习方法建议（面向初学者）特征工程开始机器学习的第一步是理解如何评估和改进数据集的质量。管理特征的类别和缺失、归一化和降维（PCA、ICA、NMF）是大幅提高算法性能的基本技术，而且还有助于研究如何将数据集分割成训练集和测试集、如何采取交叉验证来取代传统的测试方法。机器之心也曾详解过特征工程如 PCA 降维算法的详细理论与推导，当然我们还介绍了其它有关特征的概念：从特征分解到协方差矩阵：详细剖析和实现PCA算法基于TensorFlow理解三大降维技术：PCA、t-SNE 和自编码器 似乎没区别，但你混淆过验证集和测试集吗？Numpy：Python 数值计算之王！使用 Python 时，Numpy 不仅仅是一个库。它是几乎所有机器学习实现的基础，因此了解它的工作原理、关注向量化和广播（broadcasting）是非常必要的。这些技术可以帮助加速大多数算法的学习过程，利用多线程和 SIMD、MIMD 架构的力量。官方文档已经很完整了，不过，我还建议大家看一下以下资源：《Python 数据科学手册：数据使用的核心工具》，VanderPlas J. 著《Python 科学编程入门书》，LangTangen P. H. 著维度、广播操作与可视化：如何高效使用TensorFlow数据可视化Matplotlib 即使不是纯粹的机器学习话题，了解如何可视化数据集也很重要。Matplotlib 可能是最广泛使用的解决方案：Matplotlib 易用，允许绘制不同种类的图表。Bokeh 和 Seaborne 提供了有趣的替代方案。不必要彻底了解所有包，但是了解每一个包的优点和弱点还是很有用的，可以帮助你选择合适的包。了解 Matplotlib 细节的资源：《掌握 Matplotlib》，McGreggor D. 著线性回归线性回归是最简单的模型之一，可以把它作为一个优化问题来研究，该问题可通过最小化均方误差而得到求解。该方法虽然有效，但是限制了可利用的可能性。我建议还可以把它当作贝叶斯问题，使用之前的可能性展示参数（比如，高斯分布），优化变成了最大似然估计（Maximum Likelihood Estimation，MLE）。即使这看起来更加复杂，但该方法提供了一个可供几十个其他复杂模型共享的新方法。Coursera 上介绍贝叶斯统计的课程：《贝叶斯统计：从概念到数据分析》（https://www.coursera.org/learn/bayesian-statistics/）《贝叶斯统计：技术与模型》（https://www.coursera.org/learn/mcmc-bayesian-statistics）以及这两本书：《思考贝叶斯》，Downey B. A. 著《黑客的贝叶斯方法》Davidson-Pilon C. 著包括线性回归在内，机器之心曾介绍了一些解决回归问题的方法（后文提供了 CART 算法进行回归分析）：初学TensorFlow机器学习：如何实现线性回归？ 回归、分类与聚类：三大方向剖解机器学习算法的优缺点（附Python和R实现）线性分类通常情况下，Logistic 回归是最佳起始点，也是研究信息论进而了解信息熵、交叉熵和互信息的好机会。类别交叉熵（Categorical cross-entropy）是深度学习分类中最稳定、使用最广泛的代价函数，一个简单的 logistic 回归可以展示它是如何加速学习过程的（与均方差相比）。另一个重要的话题是正则化（Ridge、Lasso 和 ElasticNet）。很多情况下，人们认为它是一种提高模型准确率的深奥方式，但是它的真实意义是更准确，在具体实例的帮助下变得易于理解。我还建议刚开始的时候，把 logistic 回归当作一个简单的神经网络，可视化（以 2D 实例为例）权重向量在学习过程中的移动轨迹。我还建议本节应包括超参数网格搜索。网格搜索不在没有完整了解的情况下尝试不同的值，而是评估不同的超参数集的性能。因此，工程师可以将注意力集中在可达到最高准确率的组合上。当然还有更加强大的贝叶斯优化方法，即利用先验知识逼近未知目标函数的后验分布从而调节超参数的方法。从头开始：用Python实现带随机梯度下降的Logistic回归 如何通过牛顿法解决Logistic回归问题拟合目标函数后验分布的调参利器：贝叶斯优化支持向量机（SVM）支持向量机提供了不同的分类方法（包括线性和非线性方法）。该算法非常简单，具备基础几何知识的人也可以学会。不过，了解核支持向量机的工作原理非常有用，因为它会在线性方法失败的时候展示出其真正实力。一些有用的免费资源：《支持向量机简明教程》，Law 著核函数方法，维基百科词条详解支持向量机SVM：快速可靠的分类算法详解支持向量机（附学习资源）决策树决策树提供了另一种分类和回归的方法。通常，它们不是解决复杂问题的首选，但它们提供了完全不同的方法，即使是非技术人员也可以很容易理解，该方法还可以在会议或演示中可视化。教程 | 从头开始：用Python实现决策树算法从决策树到随机森林：树型算法的原理与实现 集成学习一览在理解了决策树的动态特性以后，研究集成训练树的集（集成）来提高整体准确率的方法很有用。随机森林、梯度树提升和 AdaBoost 都是强大的算法，且复杂度较低。对比简单的树和提升方法与 bagging 方法采用的树的学习过程挺有趣的。Scikit-Learn 提供了最常见的实现方法，但是如果你想更好地驾驭这些方法，我还是建议你在 XGBoost 上多花些时间，XGBoost 是一个既适用于 CPU 又适用于 GPU 的分布式框架，即使在较大的数据集上也能加速学习过程。从Boosting到Stacking，概览集成学习的方法与性能聚类当开始聚类方法的学习时，我的建议是从高斯混合算法（基于期望最大化/EM）学起。虽然 K-均值聚类要更加简单易懂（也是必须要学习的），但是高斯混合算法为我们提供了纯粹的贝叶斯方法，在其他类似任务中也十分实用。其它必学的算法还有层次聚类（Hierarchical Clustering）、谱聚类（Spectral Clustering）和 DBSCAN。这对你了解基于实例的学习或研究 K-近邻算法（既适用于有监督又适用于无监督任务）也是有帮助的。谱聚类的一个有用的免费资源是：《谱聚类教程》，Von Luxburg U 著聚类算法是无监督学习中的代表，机器之心也曾详细地介绍过各种聚类方法与实现：机器理解大数据的秘密：聚类算法深度详解综述分类、聚类和信息提取算法在文本挖掘领域内的应用 如何用Python和机器学习炒股赚钱？神经网络入门神经网络是深度学习的基础，你可以在单独的课程中学习神经网络。但是，我认为理解感知机、多层感知机以及反向传播算法的概念也很有帮助。Scikit-Learn 提供了一个实现神经网络的简单方法，但是，开始探索 Keras 也是一个好主意，Keras 是一个基于 Tensorflow、Theano 或 CNTK 的高级架构，允许使用最少的努力对神经网络进行建模和训练。开始神经网络学习的一些好资源：《人工神经网络基础》Hassoun M 著《Keras 深度学习》Gulli A.、 Pal S. 著目前最好的深度学习书籍可能就是：《深度学习》，Goodfellow I.、 Bengio Y.、Courville A. 著最全的DNN概述论文：详解前馈、卷积和循环神经网络技术机器之心GitHub项目：从零开始用TensorFlow搭建卷积神经网络深度神经网络全面概述：从基本概念到实际模型和硬件基础训练的神经网络不工作？一文带你跨过这37个坑 TensorFlow从基础到实战：一步步教你创建交通标志分类神经网络神经网络快速入门：什么是多层感知器和反向传播？教程 | 如何用30行JavaScript代码编写神经网络异或运算器神经网络调试手册：从数据集与神经网络说起神经网络基础：七种网络单元，四种层连接方式如何从信号分析角度理解卷积神经网络的复杂机制？神经网络架构演进史：全面回顾从LeNet5到ENet十余种架构（附论文） 麻省理工解读神经网络历史，三篇论文剖析基础理论最后，我们将介绍部分机器之心曾发过的综述性技术文章或论文，并希望这些文章能对大家全面理解各种方法有所帮助：自动驾驶计算机视觉研究综述：难题、数据集与前沿成果一文帮你发现各种出色的GAN变体深度强化学习综述：从AlphaGo背后的力量到学习资源分享从FPS到RTS，一文概述游戏人工智能中的深度学习算法视觉问答全景概述：从数据集到技术方法神经风格迁移研究概述：从当前研究到未来方向从语言学到深度学习NLP，一文概述自然语言处理 迁移学习全面概述：从基本概念到相关研究一文综述所有用于推荐系统的深度学习方法一文读懂遗传算法工作原理（附Python实现）从自编码器到生成对抗网络：一文纵览无监督学习研究现状 ", "answer_votes": "428", "answer_comment": "​15 条评论"}
{"answer_author": "匿名用户", "answer_id": 67098507, "answer_text": "互联网时代下怎样自学成data scientisthttp://datasciencemasters.org更多资源：http://dk-techlogic.blogspot.in/2012/05/best-machine-learning-resources.html?utm_medium=email&utm_source=other&utm_campaign=notifications.auto._wVEyo0LEeWu9QquNtf_Cw&m=1-----------------割线------------------再推荐一下两本书Introduction to statistical learning和Elements of statistical learning 都是斯坦福出的书，前者很基础，后者是前者的高阶版。还都有免费下载。现在正在追这本Mining of Massive Datasets又是斯坦福的，感觉难度中等偏上。", "answer_votes": "15", "answer_comment": "​4 条评论"}
{"answer_author": "知乎用户", "answer_id": 66975462, "answer_text": "泛函分析，凸优化。", "answer_votes": "7", "answer_comment": "​3 条评论"}
{"answer_author": "知乎用户", "answer_id": 68133666, "answer_text": "取决于你想学的程度，只是想混口饭吃没有太多必要花很多时间补别的东西，不用看懂证明，你知道的大概，能用软件做出个结果就够了。想认真学的话：1. 数学方面：微积分、矩阵论矩阵这一块，了解的越多对你推倒计算方面能力的提升提高非常多。当然，只想看懂不要求证明的话，本科的线性代数够用了(我指的是真的好好学线性代数...)2. 凸优化这一块的重要性非常显然了，比如你连牛顿法、梯度下降法、一维搜索等基本的凸优化都不了解 的话会非常吃力。但短期来说的话，基本上Boyd的convex optimization懂前三章就够用了。3.概率、统计对基本的期望啊mean啊之类的计算，极大似然，bayes，多元正太等很多相关方面的统计一定要比较熟悉，否则对涉及统计和对数据的直觉上会差很多。4.泛函我本科没好好学泛函，到学到一些ML的方法比如kernel相关的方法的时候就凸显出来对泛函不熟，对函数空间理解不够的话会比较吃力。但重要性上比如前面几个方面。但我整体想说的是，对于大多数只学过微积分线性代数+基本统计的人不大会有时间和精力说把上面这些一门一门学了才开始学ML，大多数时候都是慢慢去补的。比如上面有人提到Andrew Ng的coursera课程上会有一些hangouts。这是一个很好的比较快速掌握急切所需的东西的方面。毕竟上面每一门课都需要花很多时间去学。但是，如果你能真的好好学了之后，再回过头来把ML再学一遍，你收获一定会多的多！", "answer_votes": "114", "answer_comment": "​7 条评论"}
{"answer_author": "造数科技", "answer_id": 284674796, "answer_text": "造数 - 新一代智能云爬虫机器学习当然少不了各种优化算法啦！下面介绍其中一个典型的一阶算法：<img src=\"https://pic1.zhimg.com/50/v2-815807715ce6fdb03a556a344e68986a_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"555\" data-rawheight=\"307\" class=\"origin_image zh-lightbox-thumb\" width=\"555\" data-original=\"https://pic1.zhimg.com/v2-815807715ce6fdb03a556a344e68986a_r.jpg\">自我介绍：梯度下降法通常也称为最速下降法，常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型。梯度下降法以负梯度方向为搜索方向，沿梯度下降的方向求解极小值，越接近目标值，步长越小，前进越慢。迭代公式：<img src=\"https://pic4.zhimg.com/50/v2-253761c05b2c646552a268be3f282644_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"188\" data-rawheight=\"33\" class=\"content_image\" width=\"188\">其中  为步长。先问是不是，再问为什么： ：为什么用梯度下降法能收敛到极值？ ：学渣请看这里：<img src=\"https://pic4.zhimg.com/50/v2-efe58f3ff448cc6b73b181a5dcfd0147_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"522\" data-rawheight=\"235\" class=\"origin_image zh-lightbox-thumb\" width=\"522\" data-original=\"https://pic4.zhimg.com/v2-efe58f3ff448cc6b73b181a5dcfd0147_r.jpg\"> ：学霸请看这里：<img src=\"https://pic2.zhimg.com/50/v2-7a774cbe24bf825ceab3e9cd4a05b85a_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"629\" data-rawheight=\"525\" class=\"origin_image zh-lightbox-thumb\" width=\"629\" data-original=\"https://pic2.zhimg.com/v2-7a774cbe24bf825ceab3e9cd4a05b85a_r.jpg\">梯度下降法在机器学习中的应用：机器学习的三个步骤：<img src=\"https://pic1.zhimg.com/50/v2-0ff0b26572e130bcd3af567dc0b959a9_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"345\" data-rawheight=\"171\" class=\"content_image\" width=\"345\">假设我们现在已经有了几组已经完成训练的函数，现在我们尝试利用梯度下降法完成步骤3，在几组函数中挑选出最好的一个。在这我们利用损失函数(Loss Function)衡量每组参数的好坏，简单起见，我们先考虑仅有一个参数的情况：<img src=\"https://pic3.zhimg.com/50/v2-cf2d89023daf72fd462d42e84ec7e738_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"684\" data-rawheight=\"432\" class=\"origin_image zh-lightbox-thumb\" width=\"684\" data-original=\"https://pic3.zhimg.com/v2-cf2d89023daf72fd462d42e84ec7e738_r.jpg\">我们考虑损失函数  只与参数  有关，选取合适的学习速率  ，以及初始点  ，通过上面的分析我们可以发现  的下一个迭代值在其右侧，接下来重复上述步骤，直至找到一个局部极小值。这样我们就得到了最优的参数  使得总偏差量最小。<img src=\"https://pic3.zhimg.com/50/v2-434f7241bb75a6c5ce91b2658149f949_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"701\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb\" width=\"701\" data-original=\"https://pic3.zhimg.com/v2-434f7241bb75a6c5ce91b2658149f949_r.jpg\">举个栗子利用梯度下降法寻找小火龙颜值最低点：<img src=\"https://pic4.zhimg.com/50/v2-00b1e04090185afbeade669a48c70c90_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"537\" class=\"origin_image zh-lightbox-thumb\" width=\"692\" data-original=\"https://pic4.zhimg.com/v2-00b1e04090185afbeade669a48c70c90_r.jpg\">颜值函数： 选取初始值，等级  时。选取步长  。利用公式：<img src=\"https://pic4.zhimg.com/50/v2-253761c05b2c646552a268be3f282644_hd.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"188\" data-rawheight=\"33\" class=\"content_image\" width=\"188\">可以得到： , 同理： ,  ,  ...可以看到只迭代了4次就十分逼近正解啦(￣▽￣\")", "answer_votes": "26", "answer_comment": "​1 条评论"}
{"answer_author": "田star", "answer_id": 139574844, "answer_text": "优化方面，nocedal的最优化和boyd的凸优化。矩阵方面，矩阵分析六讲还不错。概率统计，能力不足够做推荐。", "answer_votes": "2", "answer_comment": "​2 条评论"}
{"answer_author": "kevin", "answer_id": 68034856, "answer_text": "这本书我觉得完全可以覆盖你说的问题了http://book.douban.com/subject/25788483/", "answer_votes": "7", "answer_comment": "​添加评论"}
{"answer_author": "Riinn", "answer_id": 67484031, "answer_text": "实际上除了微积分线代你啥都不会也能学，这课在某些专业是必修课，很多选这门课的同学数学水平[1]烂的不堪入目，当然这么上这门课痛苦很多就是了……按照我正在上机器学习的同学的说法，可能有帮助的数学知识有：数学分析实分析（测度论）泛函分析概率论数理统计随机过程凸优化博弈论（我也不知道为啥有这个……）拓扑学（这好像是那老师说着玩的……）不负任何责任。Notes：[1] 可以认为我说的是分析、代数、几何的水平。再具体一点，分析只学过微积分，最多再学过一点弱智版的复变。代数只学过线代、抽代（比较简单的那种）。几何一窍不通。", "answer_votes": "5", "answer_comment": "​3 条评论"}
{"answer_author": "江波", "answer_id": 147965839, "answer_text": "答主们已经列出了许多相关课程及其对应的教科书，但是要把一门或几门课程都系统地学习一遍无疑会耗费大量的时间和精力，同时效率也不见得高。另外这些课程中所教授的内容有相当一部分与机器学习关系不大。我自己在学校讲授一些算法课程，团队里也有同事专门从事机器学习算法和求解器的开发。在这里从优化和算法的角度出发，择出几个比较重要的知识点，供大家参考。1线性代数矩阵的各种运算要熟练（如乘积，内积，迹等），半正定矩阵的性质与各种判定条件（与凸函数的关系）。2微积分多元函数的求导（梯度，Hessian矩阵），泰勒展开，中值定理等。从我自己的教学经历来看，很多同学对多元微积分似乎有着天生的抗拒心理。我自己就见过不少已经学过微积分的同学，不会对由矩阵二次型表达的多元二次函数进行求导。其实，通过总结与一元微积分的对应关系， 完全可以做到轻松掌握多元微积分的诸多结果。3数值线性代数与线性代数相比，这门课程更偏重数值计算。其讨论的奇异值分解, Cholesky 分解, QR 分解等矩阵分解方法一般会用在主算法的子问题求解中，因此是决定程序的运行速度的关键因素之一。当然这些分解算法现今都有一些现成的软件包可以调用，但是在特定的场合，我们仍然需要对问题结构进行具体分析，提高分解算法的运行效率。4非线性规划主要是各种优化算法，大致可以分为一阶算法和二阶算法两类：一阶算法中只用到了函数的一阶导数（梯度），典型代表是梯度下降法；二阶算法还用到了函数的二阶导数（Hessian矩阵）信息，典型代表是牛顿法。世上无完事，其实每种算法都有自己的好处和弊端。例如一阶算法的优势是子问题求解的代价小，但是收效速度慢并且得到解的精度不高；另一方面，二阶算法的收敛速度很快，但是子问题的求解代价较高（一般会涉及到矩阵求逆）。为了克服上述缺点，学者们又提出了梯度法和牛顿法的诸多变种，例如：随机(stochastic)梯度法，共轭(conjugate)梯度法，邻近(proximal)梯度法，拟牛顿法等。在许多机器学习算法包或求解器中，能经常看到他们的身影。5凸分析最优性条件（大家比较熟知的是KKT条件），对偶理论。对偶理论使得我们能从另一个角度来描述原问题，从而设计一些新的算法如对偶算法，原始对偶算法 （primal-dual algorithm）等。前一段时间很火的交替方向乘子法（alternating direction method of multiplier）便是原始对偶算法的典型代表。另外就是凸优化问题的所特有的性质需要清楚，比如：局部最优等价于全局最优，强对偶定理成立，最优性条件变成了充分必要条件。6 其他一些进阶知识现在做研究的一个趋势就是做交叉，有些学者运用其他学科的工具对机器学习算法进行了研究，往往会有新的发现。因此，若要从事算法研究的话，其他数学知识当然是多多益善。比如根据算子理论可以推导出一大堆算子分裂方法，而这些方法与交替方向乘子法又有很紧密的联系；又比如机器学习大神Michael Jordan的group最近的研究工作就是用微分方程将Nesterov加速法，三次正则化方法等算法统一起来。 ", "answer_votes": "38", "answer_comment": "​5 条评论"}
{"answer_author": "腾讯云技术社区", "answer_id": 184195281, "answer_text": "针对这个问题，引用腾讯云技术社区的文章《机器学习入门书籍简介》系统性的回答下这个问题。在这篇文章中，我摘选出机器学习中涉及数学相关的书籍。其中数学可分为分析+概率，以下主要针对这两方面给出笔者看过觉得比较优质的一些书籍供大家参考，希望对你有所帮助。一、分析数学分析：首推北大张筑生版的数学分析新讲一套三册；全面深入细致讲解了数学分析的方方面面，如果觉得实数系构造这一块不够严谨，可以参考陶哲轩的实分析前面一两章；如果不求严谨，无力啃下完整的数学分析又想学习算法的同学，则推荐浙大版高等数学；再次一点可以看华中科技大出版的一元分析学和多元分析学这两本书；名字虽然叫分析学，实际比浙大高数还要简单一些。优化理论：满分推荐《最优化导论》这本书，作者是Edwin.K.P.Chong，亚马逊有中译本；这本书是我苦寻很久才找到的一本，填补了从高数到学习算法之间那一环的不二法本；第二本推荐是凸优化，不过目前只有英文版，门槛稍高，但是内容清晰简练，非常值得一读。线性代数：推荐Gilbert Strang的Introduction to linear algebra；不解释，网易上有对应的视频，满分推荐。二、概率概率论：这里推荐陈希孺的教材吧。贝叶斯：当之无愧的经典是james OBerger的《统计决策理论与贝叶斯分析》，微盘上有中文版的pdf；国内比较好的是茆诗松写的《贝叶斯统计》这本书；这里有个奇怪的现象，似乎八十年代贝叶斯在国内火过一段时间，然后就沉寂下去了，导致这块我们实际理论知之甚少，如果不是研究 lda 的时候反复查找才找到这两本书，估计我也是傻乎乎的停留在贝叶斯公式的基础上了。以上是针对机器学习中需建立的数学入门基础知识书籍的相关推荐，若想了解更多机器学习的书籍，欢迎 阅读原文，另外个人的感受就是机器学习不嫌你懂得数学多；有精力、有实力的同学可以在分析的基础上继续往上攀爬：实分析、泛函分析、微分几何、拓扑。 下面，推荐下相关的机器学习的文章： 机器学习从入门到出家 机器学习：基于层次的聚类算法【机器学习入门系列】Regression 回归：案例研究 【机器学习入门系列】 Error 的来源：偏差和方差 【机器学习入门系列】梯度下降法 【机器学习入门系列05】分类、概率生成模型 ", "answer_votes": "65", "answer_comment": "​3 条评论"}
{"answer_author": "KizW", "answer_id": 142686266, "answer_text": "1.英语2.数学线性代数 linear algebra and its applications (看过lay写的还不错据说gilbert写的更好，国内蓝以中 高等代数简明教程 也不错，看上册就够了,下册是抽代，蓝以中这本对于机器学习来讲很不错，提了行列式求导和正定二次型，线性代数的书一般不讲这块)高等数学 数学分析新讲 张筑生 (第三册看看级数，国外的据说托马斯微积分不错 没看过。矩阵向量求导是著名的没书讲领域，貌似很多人在这里上火，一篇深度美文 机器学习中的矩阵，向量求导)统计 概率论和数理统计 陈希孺 (至少看完前四章)多元统计 applied multivariate statiatical analysis richard johnson (至少看完前四章 pca和factor analysis也应该看一看，其实pca和fa在机器学习中本身就是无监督学习的算法)最优化 convex optimization stephen boyd (至少看1-5 9 章，看完第五章的时候看一下pattern recognization and machine learning这本书附录的拉格朗日算子，这一章是svm的基础)珍爱生命，远离rudin我看完这些看的prml觉得没什么问题。手头还有周志华和李航的书，建议当做补充材料，不太适合建立知识架构。数学进阶:(入门用不上的)蒙特卡洛 introducing monte carlo methods with r Christian Robert（prml里面11章就是讲抽样方法的，不妨先看下）拓扑 topology without tears sidney a. morris泛函 introductory functional analysis with applications kreyszig流形还有李群李代数没看过3.机器学习pattern recognization and machine learning Bishopthe elements of statistical learning hastie（据说挺难的 没看过）machine learning yearning 吴恩达（面向工程的小册子，没看过，不过吴恩达写的，不会有问题）4.深度学习deep learning lan goodfellow(面向工程的，有难度，全搞懂估计要对着里面给的论文看)neural networks and deep learning (太简单)http://neuralnetworksanddeeplearning.com5.编程python(python核心编程)主 c++(c++ primer 5 lippman)为辅 python那几个库 底层的scipy numpy matplotlib sympy(python科学计算 张若愚)，机器学习的scikit-learn(scikit-learn cookbook)，深度学习的tensorflow(tensorflow实战 黄文坚)，数据库的(expert oracle database architecture thomas),图像识别还要用图像处理(数字图像处理 gonzalez)的那些东西opencv cuda什么的建了一个小群，大家互相帮助嘛 看书看不懂了一起研究，有好资料一起分享 qq群436142301", "answer_votes": "4", "answer_comment": "​1 条评论"}
{"answer_author": "大野人007", "answer_id": 139408269, "answer_text": "如果真心希望学好机器学习，那么数学自然是非常有必要的，这边我提供一个个非常好的机器学习路线的培养方案，JustFollowUs/Machine-Learning，可以看看您还缺什么数学知识，这个github上面的内容基本是机器学习必备的，所以建议一步一步的全都学完，因为基本每个课程都提供了大家公认的好视频，能帮助更好的学习。当然如果仅仅只是希望找个工作过日子，这个上面的资料可能不太适合您，请忽略这条回答。", "answer_votes": "115", "answer_comment": "​6 条评论"}
{"answer_author": "知乎用户", "answer_id": 263038297, "answer_text": "1、线性代数和矩阵理论，具体到机器学习里面就是线性变换，svd之类，pca和lda都会用到；2、高等数学，具体到机器学习就是求导，链式法则，积分思想等等；3、优化理论，有两本书不错《Convex Optimization》和《Numerical Optimization》，具体到机器学习就是给定目标函数，求解参数的过程，什么拉格朗日乘数法，对偶问题，kkt条件，梯度下降法，牛顿法，拟牛顿法等等。4、概率与统计，机器学习的模型多与概率，统计有一些关系，比如概率图模型。这是最基础的，其他要用到的数学知识，碰到了再去学。", "answer_votes": "23", "answer_comment": "​2 条评论"}
{"answer_author": "cstghitpku", "answer_id": 67231770, "answer_text": "矩阵论、概率论、凸优化、微积分、梯度", "answer_votes": "5", "answer_comment": "​添加评论"}
{"answer_author": "以史为贱", "answer_id": 101290600, "answer_text": "进入特定技能之前，要先解决多一个的概念。作为一个机器学习的工程师就必须了解整个生态系统，你的设计和  语言和库的机器学习。要跳到这个作业中有一个需要具备以下技能：1.计算机科学基础知识和编程2.概率统计3.数据模拟和评价4.将机器学习算法与程序库5.软件工程和系统设计", "answer_votes": "11", "answer_comment": "​2 条评论"}
{"answer_author": "妮妮蔡", "answer_id": 134965944, "answer_text": "最基础的部分包括基本的高等数学，比如分析、代数（尤其是矩阵论）、数值优化算法、概率论与数理统计等。更进阶的需要掌握实分析（比如测度论）、图论、时间序列、回归分析等等。再深入的你还可以掌握微分方程、流形几何等等基础机器学习涉及不到的内容，这个时候你就可以挖别人挖不出来的坑了。另外虽然严格来说不属于数学，但是算法的概念和数据结构的相关知识也是一定要掌握的", "answer_votes": "3", "answer_comment": "​添加评论"}
{"answer_author": "Zooee", "answer_id": 68942379, "answer_text": "李航博士的《统计学习方法不错》", "answer_votes": "3", "answer_comment": "​添加评论"}
{"answer_author": "IT168企业级", "answer_id": 232881293, "answer_text": "机器学习是计算机科学的一个子领域，使计算机能够在没有被明确编程的情况下自主学习，它探讨了从数据中学习和预测的算法研究和构建。机器学习的范围广泛，跨越数学、计算机科学和神经科学等多个领域。　　该路线图也有一个Jupyter notebook，记载着大部分Data Science步骤，可以在以下链接中找到：https：//http://github.com/dformoso/sklearn-classification，感谢Github用户dformoso的分享!　　数据科学是一个需要设计、实施和维护的过程。部分路线图如下所示：<img src=\"https://pic4.zhimg.com/50/v2-82d156def6d7c17d506aaf5fce04631e_hd.jpg\" data-rawwidth=\"640\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-82d156def6d7c17d506aaf5fce04631e_r.jpg\">　　数据部分　　首先，我们需要一些数据，找到数据、收集数据、清理数据等共5步。<img src=\"https://pic2.zhimg.com/50/v2-d166e4fd2af51d9db8b056014b194b47_hd.jpg\" data-rawwidth=\"640\" data-rawheight=\"404\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-d166e4fd2af51d9db8b056014b194b47_r.jpg\">　　数学部分　　机器学习是在数学基础上建立起来的!我们需要了解一些函数知识!<img src=\"https://pic2.zhimg.com/50/v2-6771279db634ed77e909c1d41d2cc7e2_hd.jpg\" data-rawwidth=\"640\" data-rawheight=\"398\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-6771279db634ed77e909c1d41d2cc7e2_r.jpg\">　　概念部分　　类别，方法，库和方法的部分列表。<img src=\"https://pic3.zhimg.com/50/v2-0a371e28ec16b84b52c85fd915f3deba_hd.jpg\" data-rawwidth=\"640\" data-rawheight=\"335\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-0a371e28ec16b84b52c85fd915f3deba_r.jpg\">　　模型部分　　最受欢迎的模型抽样。<img src=\"https://pic4.zhimg.com/50/v2-49129ee8d91aaaca03c8d68e601faffa_hd.jpg\" data-rawwidth=\"640\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-49129ee8d91aaaca03c8d68e601faffa_r.jpg\">　　整个机器学习和深度学习路线图非常庞大，因此此处无法提供完整视图，完整路线图可到http://wenku.it168.com/d_001728559.shtml和http://wenku.it168.com/d_001728560.shtml中下载或查看!对于机器学习而言，并不存在“一体适用”的说法。构建不同的模型需要用到不同的算法和不同技术，但如何才能确定哪种技术最合适呢?机器学习并不局限于一套具体的算法，它依赖于你想要达到的目标，或者说依赖于数据类型和数据量。以下是机器学习中一些常见的技术：监督式学习数据是否被贴上标签，决定了它是被监督还是无人监督的。监督学习使用人类标记过的数据，通常在数据有能力预测可能发生的事件时使用。换句话说，在我们知道自己需要得到什么样的结果输出时，所输入数据就是这些标记过的数据。该算法对一组输入数据进行处理，并得到相应的输出结果，通过将该输出结果与正确结果进行比较来发现错误。一旦发现错误，就可以相应地对模型进行修改。分类分类属于监督学习，可以定义为尝试预测给定输入的输出结果。分类采用一组未知的实体，将其识别为更大的已知组。为了进行学习，它需要一组已标记的实例，例如图像、文本或语音。随着训练次数的增长，使分类器达到高准确度所需的数据量可能很大，达到数千甚至数百万个实例。虽然分类通常针对简单的类别，但它可以扩展到目标是一个结构或一个序列的情况，就像自然语言处理一样。无监督学习无监督学习使用无标记的数据。在这种情况下，机器在不了解任何事先数据或信息的情况下发现了新的模式。当数据被归类到类似的数据组时，这种类型的学习会在集群下很好地工作。强化学习受强化行为心理观念的启发，强化学习是一种学习的理念。机器可以通过多次尝试和错误来获得一个理想的结果。随着时间的推移，它学会了选择某种行为，以得到理想的结果。这种类型的学习经常被应用于游戏、导航等应用程序中。神经网络深度神经网络(DNNs)，又称人工神经网络(ANN)，代表了一套用于构建强大的学习系统的技术。与某些算法不同，它们添加了一些“隐藏”层，用于提取中间表示。这种技术于20世纪80年代被发明，在2010年之后开始腾飞，这种飞速增长得益于强大的并行硬件和易于使用的开源软件。DNNs覆盖了一系列不同的神经体系结构，最著名的是:递归神经网络(RNN)——神经元将反馈信号发送给彼此卷积神经网络(CNN)——前馈神经网络，通常用于视觉和图像识别", "answer_votes": "2", "answer_comment": "​添加评论"}
{"answer_author": "Mio", "answer_id": 137023203, "answer_text": "主要包括：最优化理论，图论，概率论，随机过程，矩阵论，泛函分析等。建议听一下Andrew Ng的网易公开课。", "answer_votes": "2", "answer_comment": "​添加评论"}
{"answer_author": "匿名用户", "answer_id": 239242230, "answer_text": "1.基础篇:微积分、线性代数、概率统计（既考研数学一要求的全部内容）2.深入篇:矩阵分析、凸优化；泛函分析、随机过程3.附加:多元统计、非参数统计。这两门课是模式识别机器学习里面很多算法的来源，可以看看加深理解，没必要系统去学。4.如果涉及到某些特殊方向，还要学一下非负矩阵和张量分解。（在学校里面，这些课程一般都分为数学系和工科版两种，数学系的只适合数学专业的某些方向去学。另外大学里面的应用性质的随机和泛函也比较偏电子通信，对计算机专业的同学很不友好。）", "answer_votes": "2", "answer_comment": "​1 条评论"}
{"answer_author": "匿名用户", "answer_id": 235845988, "answer_text": "给大家分享个1000+ 的机器学习与人工智能资料、论文、教学视频、数学基础、入门博客整理合集以及开发框架介绍。分享自: 机器学习专题 - DevOpen.Club | 高质量的软件开发视频教程", "answer_votes": "1", "answer_comment": "​添加评论"}
{"answer_author": "梁捷海", "answer_id": 140086352, "answer_text": "概率论【重点是贝叶斯概率论，不是抛硬币猜正反面那一套】算法优化算法", "answer_votes": "1", "answer_comment": "​添加评论"}
{"answer_author": "Tony利兵", "answer_id": 66998056, "answer_text": "一直在看prml， 也曾经很纠结里面的公式推导，感觉难度最大的一块在于一些矩阵求导相关的内容(假设本科已经学过高等数学，线性代数的)，建议先细致看一下matrix calc，自己把一些矩阵求导的公式推一遍。", "answer_votes": "1", "answer_comment": "​2 条评论"}
{"answer_author": "知乎用户", "answer_id": 66977913, "answer_text": "线性代数：《 Introduction to Linear Algebra》概率论：《Applied multivariate statistical analysis》 《Probability Theory The Logic of Science》统计：《All of Statistics》其他的泛函，凸优化需要的时候再去查相关知识最近也在看prml，感觉理解频率派跟贝叶斯学派的差别很重要", "answer_votes": "1", "answer_comment": "​添加评论"}
{"answer_author": "熊本一身白", "answer_id": 262816505, "answer_text": "简单说是按需学习，如果连sklearn包里中最常见的模型调参都不会，然后去啃各种复杂的公式，然后感觉太难了，不学了。你说只是为了订个钉子却抱着一本教你如何制造锤子的书，是不是有点儿大材小用了。当然，如果要进阶或者做data engineering的工作，数学和统计就特别重要了。", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "Xiecheng", "answer_id": 256134890, "answer_text": "简单答一下。想要入坑机器学习/深度学习，需要的数学知识可以参考各个大学本科数学/统计的教学大纲。按顺序来基本上是：高等代数数学分析概率论回归分析多元统计分析学习高代数分对理解各种算法是很有必要的。概率论回归多元偏统计分析那一块，可以培养些统计的思维。非科班的来说可以我建议至少高代和概率论是需要精读的，其他几门课可以泛读或者跟着网上课程过一遍。有一本书可以推荐给大家，The Elements of Statistical Learning。网上有作者们放出的免费电子版，前段时间定期都会有更新。", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "匿名用户", "answer_id": 246050621, "answer_text": "数学分析概率论数理统计线性代数最优化理论随机过程矩阵分析数值分析。。。（如果要做科研的话，看多多数学书都不够）", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "知乎用户", "answer_id": 233937954, "answer_text": "微积分 高代 统计 数值方法", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "则安", "answer_id": 233159760, "answer_text": "我们学的是经济数学基础然后我们的专业上图&lt;img src=\"https://pic4.zhimg.com/50/v2-4434b63986e057598f811f2a2401830a_hd.jpg\" data-rawwidth=\"1536\" data-rawheight=\"2048\" class=\"origin_image zh-lightbox-thumb\" width=\"1536\" data-original=\"https://pic4.zhimg.com/v2-4434b63986e057598f811f2a2401830a_r.jpg\"&gt;&lt;img src=\"https://pic3.zhimg.com/50/v2-8a6c813ea5095fe1eea4b2d355423ba0_hd.jpg\" data-rawwidth=\"1536\" data-rawheight=\"2048\" class=\"origin_image zh-lightbox-thumb\" width=\"1536\" data-original=\"https://pic3.zhimg.com/v2-8a6c813ea5095fe1eea4b2d355423ba0_r.jpg\"&gt;", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "忘了忘不了", "answer_id": 233040708, "answer_text": "Caltech - learning from dataItunesU 同edX 都有", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "无所不能的小裁缝", "answer_id": 210705033, "answer_text": "机器学习理论是统计学、概率学、计算机科学以及算法的交叉领域，是通过从数据中的迭代学习去发现能够被用来构建智能应用的隐藏知识。尽管机器学习和深度学习有着无限可能，然而为了更好地掌握算法的内部工作机理和得到较好的结果，对大多数这些技术有一个透彻的数学理解是必要的。机器学习中的数学是重要的，有很多原因，下面强调其中的一些：1. 选择正确的算法，包括考虑到精度、训练时间、模型复杂度、参数的数量和特征数量。2. 选择参数的设置和验证策略。3. 通过理解偏差和方差之间的 tradeoff 来识别欠拟合与过拟合。4. 估计正确的置信区间和不确定度。需要重视的数学内容：1. 线性代数：在机器学习领域，线性代数无处不在。主成分分析（PCA）、奇异值分解（SVD）、矩阵的特征分解、LU 分解、QR 分解、对称矩阵、正交化和正交归一化、矩阵运算、投影、特征值和特征向量、向量空间和范数（Norms），这些都是理解机器学习中所使用的优化方法所需要的。2. 概率论和统计学：机器学习和统计学并不是迥然不同的领域。事实上，最近就有人将机器学习定义为「在机器上做统计」。机器学习需要的一些概率和统计理论分别是：组合、概率规则和公理、贝叶斯定理、随机变量、方差和期望、条件和联合分布、标准分布（伯努利、二项式、多项式、均匀和高斯）、时刻生成函数（Moment Generating Functions）、最大似然估计（MLE）、先验和后验、最大后验估计（MAP）和抽样方法。3. 多元微积分：一些必要的主题包括微分和积分、偏微分、向量值函数、方向梯度、海森、雅可比、拉普拉斯、拉格朗日分布。4. 算法和复杂优化：这对理解我们的机器学习算法的计算效率和可扩展性以及利用我们的数据集中稀疏性很重要。需要的知识有数据结构（二叉树、散列、堆、栈等）、动态规划、随机和子线性算法、图论、梯度/随机下降和原始对偶方法。5. 其他：这包括以上四个主要领域没有涵盖的数学主题。它们是实数和复数分析（集合和序列、拓扑学、度量空间、单值连续函数、极限）、信息论（熵和信息增益）、函数空间和流形学习。<img src=\"https://pic3.zhimg.com/50/v2-c707279f20691483e45745aeb8237317_hd.jpg\" data-rawwidth=\"638\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb\" width=\"638\" data-original=\"https://pic3.zhimg.com/v2-c707279f20691483e45745aeb8237317_r.jpg\">一些机器学习的痴迷者是数学新手，看了这些后可能会比较失望。对于初学者而言，其实你并不需要很多的数学知识就能够开始机器学习的研究。你可以在掌握更多的技术和算法的过程中学习数学。", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "doddle", "answer_id": 184442311, "answer_text": "着重看反向传播算法中使用的链式法则，另外对矩阵乘法规则好好摸索吃透基本上就差不多了，至于什么贝叶斯，奇异值分解什么的可以慢慢来", "answer_votes": "0", "answer_comment": "​添加评论"}
{"answer_author": "知乎用户", "answer_id": 184267376, "answer_text": "我姑且理解题中的机器学习是指统计机器学习。首先你要了解函数，函数的奇偶性，收敛性。一个不收敛的模型是无法做优化的。其次你还要了解极限，数列，微积分。然后是向量，矩阵，矩阵的内积，范数，秩，矩阵的求导和运算，矩阵的正定性。概率统计上你要了解条件概率，先验概率，后验概率（贝叶斯），期望，方差，二项分布，泊松分布。基本上都是高中到大学的基础。有了这些基础你去看数学分析和优化的书基本上没什么难度。", "answer_votes": "0", "answer_comment": "​添加评论"}
